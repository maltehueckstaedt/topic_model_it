{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da160223",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "## Pakete und Daten laden\n",
    "\n",
    "In einem ersten Schritt laden wir die nöten Pakete und laden die Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800a438-5e88-4c45-a726-bda7eeecac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import os\n",
    "import openpyxl\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from pathlib import Path\n",
    "\n",
    "target = Path(\"C:/Users/mhu/Documents/github/topic_model_it\")\n",
    "os.chdir(target)\n",
    "\n",
    "data_model_1 = pd.read_csv(\"data/informatikkurse.csv\")\n",
    "print(data_model_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ae8bd",
   "metadata": {},
   "source": [
    "## Reproduzierbarkeit durch Seeds\n",
    "Im folgenden fixieren wir die Zufallszahlen-Generatoren aller beteiligten Bibliotheken auf den Wert 11. Dies stellt sicher, dass die Experimente bei jedem Durchlauf identische Ergebnisse liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9a8dc-d866-44cf-86bc-e26fca5c9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11  # Initialisiert den Seed-Wert für reproduzierbare Ergebnisse\n",
    "np.random.seed(seed)  # Setzt den Seed für NumPy-Zufallszahlengeneratoren\n",
    "random.seed(seed)  # Setzt den Seed für den Python-eigenen Zufallszahlengenerator\n",
    "torch.manual_seed(seed)  # Setzt den Seed für PyTorch-Zufallszahlen\n",
    "if torch.cuda.is_available():  # Überprüft, ob CUDA (GPU-Unterstützung) verfügbar ist\n",
    "    torch.cuda.manual_seed_all(seed)  # Setzt den Seed für alle CUDA-Zufallszahlen (für GPU-Berechnungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e14ed",
   "metadata": {},
   "source": [
    "## Docs\n",
    "\n",
    "Wir generieren als nächsten die Dokumente, die das Eingangsmaterial für das Topic-Model bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige der Spaltennamen von data_model_1\n",
    "print(data_model_1.columns.tolist())\n",
    "\n",
    "data_model_1 = data_model_1[[\"veranstaltung_titel\", \"kursbeschreibung\"]].copy()\n",
    "data_model_1.head()\n",
    "\n",
    "data_model_1 = data_model_1.apply(lambda x: x.fillna('') if x.dtype == 'O' else x)  # Ersetzt fehlende Werte durch leere Strings in Objektspalten (Strings) und belässt numerische Spalten unverändert\n",
    "data_model_1['titel_kursbesch'] = data_model_1['veranstaltung_titel'] + ' ' + data_model_1['kursbeschreibung']  # Kombiniert die Spalten \"titel\" und \"kursbeschreibung\" zu einer neuen Spalte \"titel_kursbesch\"\n",
    "docs = data_model_1['titel_kursbesch'].tolist()  # Konvertiert die Inhalte der Spalte \"titel_kursbesch\" in eine Liste von Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cac53-81bc-4784-8716-0108d3bfdf31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d74ac8-6f38-4603-8d4c-d0e7d33be06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stopwords_config\n",
    "\n",
    "irrelevant_terms = stopwords_config.irrelevant_terms\n",
    "\n",
    "sw = list(stopwords.get_stopwords(\"en\"))\n",
    "sw.extend(list(stopwords.get_stopwords(\"de\")))\n",
    "sw.extend(irrelevant_terms)\n",
    "irrelevant_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4740b1",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Wir definieren unser Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d14e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7373be7-520a-4bc9-84cf-2b6e25ef95bd",
   "metadata": {},
   "source": [
    "## Model-Settings, GPU-Check\n",
    "Konfiguration frei wählbar (einfach im Code unten anpassen).\n",
    "Hier kann eine erste explorative Untersuchung durchgeführt werden.\n",
    "\n",
    "Wir checken weiterhin, ob wirklich unsere GPU verwendet wird. Wenn `cuda:0` ist dies der Fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52a78b-fa5d-4d36-8c40-86f509ba02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "  stop_words=sw,  # Entfernt Stopwörter basierend auf der angegebenen Liste (sw)\n",
    "  token_pattern=r'\\b\\w+\\b',  # Extrahiert nur ganze Wörter, d. h. keine Sonderzeichen oder Zahlen\n",
    "  ngram_range=(1, 3)  # Erstellt 1-Gramme (einzelne Wörter) bis 3-Gramme (Wortgruppen aus bis zu 3 aufeinanderfolgenden Wörtern)\n",
    ")\n",
    "\n",
    "# Embedding Settings  \n",
    "embedding_model = SentenceTransformer(\n",
    "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# UMAP Settings\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10,\n",
    "    n_components=10,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=seed\n",
    ")\n",
    "# HDBSCAN Settings\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    cluster_selection_epsilon=0.2,\n",
    "    prediction_data=True\n",
    ")\n",
    "# Representation Settings\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.1)\n",
    "\n",
    "# BERTopic initialisieren\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  #min_topic_size=10,\n",
    "  nr_topics=20, \n",
    "  language=\"multilingual\",\n",
    "  verbose=True,\n",
    "  umap_model=umap_model,\n",
    "  vectorizer_model=vectorizer,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  top_n_words = 15,\n",
    "  representation_model=representation_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99727b96-036b-4d95-81d0-93f224e36335",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Wir trainieren das Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17203a0-5422-4148-98a9-8242e72155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_1 = topic_model.fit(docs)\n",
    "topic_model_1.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd4c44",
   "metadata": {},
   "source": [
    "## Outlier Reduzieren, Topics mergen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.merge_topics(docs, [0, 13])\n",
    "\n",
    "# Outlier reduzieren\n",
    "# BERTopic auf Test-Daten anwenden\n",
    "# topics, probs = topic_model_1.transform(docs)\n",
    "# print(topic_model_1.get_topic_freq())\n",
    "\n",
    "# # Outlier reduzieren\n",
    "# topics = topic_model_1.reduce_outliers(docs, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_1.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caae30",
   "metadata": {},
   "source": [
    "## Labeling\n",
    "\n",
    "Mit Hilfe von gpt4o lassen wir uns die Topcis labeln. Das erleichtert die Interpretation des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e56f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI as OpenAIClient\n",
    "from bertopic.representation import OpenAI as OpenAIRep\n",
    "\n",
    "client = OpenAIClient()  # nutzt OPENAI_API_KEY aus der Umgebung\n",
    "rep = OpenAIRep(\n",
    "    client=client, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    delay_in_seconds=10,      # Erhöhe Delay\n",
    "    exponential_backoff=True, # Aktiviere das schrittweise längere Warten\n",
    "    nr_docs=3                 # Weniger Dokumente pro Topic reduzieren die Token-Last\n",
    ")\n",
    "topic_model.update_topics(docs, representation_model=rep)\n",
    "topic_model.get_topic_info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26201712",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern\n",
    "topic_model.save(\"models/explor_model_1\", serialization=\"safetensors\", save_embedding_model=True)\n",
    "\n",
    "# Laden\n",
    "from bertopic import BERTopic\n",
    "loaded_model = BERTopic.load(\"models/explor_model_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
