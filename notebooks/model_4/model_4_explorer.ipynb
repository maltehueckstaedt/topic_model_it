{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da160223",
   "metadata": {},
   "source": [
    "# Model 4\n",
    "## Pakete und Daten laden\n",
    "\n",
    "In einem ersten Schritt laden wir die nöten Pakete und laden die Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b800a438-5e88-4c45-a726-bda7eeecac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import os\n",
    "import openpyxl\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path to the utils directory (it's in the root, not src/utils)\n",
    "utils_path = Path(r\"C:\\Users\\mhu\\Documents\\github\\topic_model_it\\utils\")\n",
    "if str(utils_path) not in sys.path:\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "# Now import the module\n",
    "from utils import calculate_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f119e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5021\n",
      "Benötigte Samples zum Labeln: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhu\\AppData\\Local\\Temp\\ipykernel_21808\\1869456471.py:7: DtypeWarning: Columns (15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  topic_model_5 = pd.read_csv(\"data/informatikkurse.csv\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "target = Path(\"C:/Users/mhu/Documents/github/topic_model_it\")\n",
    "os.chdir(target)\n",
    "\n",
    "topic_model_5 = pd.read_csv(\"data/informatikkurse.csv\")\n",
    "topic_model_5 = topic_model_5[topic_model_5['model_4'] == 1].copy()\n",
    "\n",
    "len_data = len(topic_model_5)\n",
    "print(len_data)\n",
    "result = calculate_sample_size(N=len_data, p=0.9, z=1.96, e=0.03)\n",
    "print(f\"Benötigte Samples zum Labeln: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ae8bd",
   "metadata": {},
   "source": [
    "## Reproduzierbarkeit durch Seeds\n",
    "Im folgenden fixieren wir die Zufallszahlen-Generatoren aller beteiligten Bibliotheken auf den Wert 11. Dies stellt sicher, dass die Experimente bei jedem Durchlauf identische Ergebnisse liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9a8dc-d866-44cf-86bc-e26fca5c9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11  # Initialisiert den Seed-Wert für reproduzierbare Ergebnisse\n",
    "np.random.seed(seed)  # Setzt den Seed für NumPy-Zufallszahlengeneratoren\n",
    "random.seed(seed)  # Setzt den Seed für den Python-eigenen Zufallszahlengenerator\n",
    "torch.manual_seed(seed)  # Setzt den Seed für PyTorch-Zufallszahlen\n",
    "if torch.cuda.is_available():  # Überprüft, ob CUDA (GPU-Unterstützung) verfügbar ist\n",
    "    torch.cuda.manual_seed_all(seed)  # Setzt den Seed für alle CUDA-Zufallszahlen (für GPU-Berechnungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e14ed",
   "metadata": {},
   "source": [
    "## Docs\n",
    "\n",
    "Wir generieren als nächsten die Dokumente, die das Eingangsmaterial für das Topic-Model bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige der Spaltennamen von data_model_4\n",
    "print(data_model_4.columns.tolist())\n",
    "\n",
    "data_model_4 = data_model_4[[\"veranstaltung_titel\", \"kursbeschreibung\"]].copy()\n",
    "data_model_4.head()\n",
    "\n",
    "data_model_4 = data_model_4.apply(lambda x: x.fillna('') if x.dtype == 'O' else x)  # Ersetzt fehlende Werte durch leere Strings in Objektspalten (Strings) und belässt numerische Spalten unverändert\n",
    "data_model_4['titel_kursbesch'] = data_model_4['veranstaltung_titel'] + ' ' + data_model_4['kursbeschreibung']  # Kombiniert die Spalten \"titel\" und \"kursbeschreibung\" zu einer neuen Spalte \"titel_kursbesch\"\n",
    "docs = data_model_4['titel_kursbesch'].tolist()  # Konvertiert die Inhalte der Spalte \"titel_kursbesch\" in eine Liste von Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cac53-81bc-4784-8716-0108d3bfdf31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d74ac8-6f38-4603-8d4c-d0e7d33be06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stopwords_config\n",
    "\n",
    "irrelevant_terms = stopwords_config.irrelevant_terms\n",
    "\n",
    "sw = list(stopwords.get_stopwords(\"en\"))\n",
    "sw.extend(list(stopwords.get_stopwords(\"de\")))\n",
    "sw.extend(irrelevant_terms)\n",
    "irrelevant_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4740b1",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Wir definieren unser Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d14e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7373be7-520a-4bc9-84cf-2b6e25ef95bd",
   "metadata": {},
   "source": [
    "## Model-Settings, GPU-Check\n",
    "Konfiguration frei wählbar (einfach im Code unten anpassen).\n",
    "Hier kann eine erste explorative Untersuchung durchgeführt werden.\n",
    "\n",
    "Wir checken weiterhin, ob wirklich unsere GPU verwendet wird. Wenn `cuda:0` ist dies der Fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52a78b-fa5d-4d36-8c40-86f509ba02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "  stop_words=sw,  # Entfernt Stopwörter basierend auf der angegebenen Liste (sw)\n",
    "  token_pattern=r'\\b\\w+\\b',  # Extrahiert nur ganze Wörter, d. h. keine Sonderzeichen oder Zahlen\n",
    "  ngram_range=(1, 3)  # Erstellt 1-Gramme (einzelne Wörter) bis 3-Gramme (Wortgruppen aus bis zu 3 aufeinanderfolgenden Wörtern)\n",
    ")\n",
    "\n",
    "# Embedding Settings  \n",
    "embedding_model = SentenceTransformer(\n",
    "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# UMAP Settings\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10,\n",
    "    n_components=10,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=seed\n",
    ")\n",
    "# HDBSCAN Settings\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=100,\n",
    "    cluster_selection_epsilon=0.2,\n",
    "    prediction_data=True\n",
    ")\n",
    "# Representation Settings\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.1)\n",
    "\n",
    "# BERTopic initialisieren\n",
    "data_model_4 = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  # min_topic_size=100,\n",
    "  nr_topics=\"auto\", \n",
    "  language=\"multilingual\",\n",
    "  verbose=True,\n",
    "  umap_model=umap_model,\n",
    "  vectorizer_model=vectorizer,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  top_n_words = 20,\n",
    "  representation_model=representation_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99727b96-036b-4d95-81d0-93f224e36335",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Wir trainieren das Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17203a0-5422-4148-98a9-8242e72155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_4.fit(docs)\n",
    "data_model_4.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd4c44",
   "metadata": {},
   "source": [
    "## Topic Visualiszieren, Outlier Reduzieren, Topics mergen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_4.visualize_topics()\n",
    "#data_model_4.visualize_hierarchy() # Hilft extrem zu sehen, welche Themen zusammengehören"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.merge_topics(docs, [0, 13])\n",
    "\n",
    "# # Outlier reduzieren\n",
    "# # BERTopic auf Test-Daten anwenden\n",
    "# topics, probs = data_model_4.transform(docs)\n",
    "# print(data_model_4.get_topic_freq())\n",
    "\n",
    "# # # Outlier reduzieren\n",
    "# topics = data_model_4.reduce_outliers(docs, topics)\n",
    "# data_model_4.update_topics(docs, topics=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model_4.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caae30",
   "metadata": {},
   "source": [
    "## Labeling\n",
    "\n",
    "Mit Hilfe von gpt4o lassen wir uns die Topcis labeln. Das erleichtert die Interpretation des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e56f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI as OpenAIClient\n",
    "from bertopic.representation import OpenAI as OpenAIRep\n",
    "\n",
    "client = OpenAIClient()  # nutzt OPENAI_API_KEY aus der Umgebung\n",
    "rep = OpenAIRep(\n",
    "    client=client, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    delay_in_seconds=10,      # Erhöhe Delay\n",
    "    exponential_backoff=True, # Aktiviere das schrittweise längere Warten\n",
    "    nr_docs=3                 # Weniger Dokumente pro Topic reduzieren die Token-Last\n",
    ")\n",
    "data_model_4.update_topics(docs, representation_model=rep)\n",
    "data_model_4.get_topic_info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern\n",
    "topic_model.save(\"models/explor_model_1\", serialization=\"safetensors\", save_embedding_model=True)\n",
    "\n",
    "# Laden\n",
    "from bertopic import BERTopic\n",
    "loaded_model = BERTopic.load(\"models/explor_model_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
