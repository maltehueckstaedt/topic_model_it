{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d2dc7e",
   "metadata": {},
   "source": [
    "# Template für Topic Modeling\n",
    "Dieses Template soll dabei helfen, Topic Modeling automatisiert und einheitlich durchzuführen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a5f4",
   "metadata": {},
   "source": [
    "## Allgemeine Vorbereitungsschritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634674f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktuelles Arbeitsverzeichnis anzeigen und bei Bedarf anpassen\n",
    "# print(os.getcwd())\n",
    "# os.chdir(\"C:/SV/HEX/Topic Modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d61603",
   "metadata": {},
   "source": [
    "### Pakete laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1471af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import os\n",
    "import openpyxl\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic.representation import MaximalMarginalRelevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feeb504",
   "metadata": {},
   "source": [
    "### Seed setzen\n",
    "Wir setzen einen festen Seed, um Zufallszahlen in NumPy und PyTorch reproduzierbar zu machen, sowohl auf der CPU als auch auf der GPU (falls verfügbar). Das stellt sicher, dass Berechnungen mit zufälligen Operationen bei wiederholter Ausführung dieselben Ergebnisse liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d700e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40  # Initialisiert den Seed-Wert für reproduzierbare Ergebnisse\n",
    "np.random.seed(seed)  # Setzt den Seed für NumPy-Zufallszahlengeneratoren\n",
    "random.seed(seed)  # Setzt den Seed für den Python-eigenen Zufallszahlengenerator\n",
    "torch.manual_seed(seed)  # Setzt den Seed für PyTorch-Zufallszahlen\n",
    "if torch.cuda.is_available():  # Überprüft, ob CUDA (GPU-Unterstützung) verfügbar ist\n",
    "    torch.cuda.manual_seed_all(seed)  # Setzt den Seed für alle CUDA-Zufallszahlen (für GPU-Berechnungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99ec26",
   "metadata": {},
   "source": [
    "### Datensätze einlesen\n",
    "Der Trainings- und der Test-Datensatz werden hier eingelesen. Als Faustregel gilt, der Trainingsdatensatz sollte 80% und der Test-Datensatz 20% des Volumens ausmachen. \n",
    "Der Trainings-Datensatz wird für das trainieren / fitten des Modells verwendet. Der Test-Datensatz beinhaltet eine (in diesem Fall manuell erstellte) sogenannte \"Ground Truth\". Dies ist der Goldstandard, anhand dessen das Modell auf Performance hin überprüft wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1c5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training-Datensatz\n",
    "training_set = pd.read_csv(\"data/processed/train_data.csv\")  # Liest die CSV-Datei ein und speichert sie in einem DataFrame\n",
    "# training_set = training_set.sample(n=500, random_state=42)  # Zieht eine Zufallsstichprobe von 500 Zeilen aus dem DataFrame mit festgelegtem Seed für Reproduzierbarkeit\n",
    "training_set = training_set.apply(lambda x: x.fillna('') if x.dtype == 'O' else x)  # Ersetzt fehlende Werte durch leere Strings in Objektspalten (Strings) und belässt numerische Spalten unverändert\n",
    "training_set['titel_kursbesch'] = training_set['veranstaltung_titel'] + ' ' + training_set['kursbeschreibung']  # Kombiniert die Spalten \"titel\" und \"kursbeschreibung\" zu einer neuen Spalte \"titel_kursbesch\"\n",
    "docs = training_set['titel_kursbesch'].tolist()  # Konvertiert die Inhalte der Spalte \"titel_kursbesch\" in eine Liste von Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10289785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Datensatz\n",
    "test_data = pd.read_csv(\"C:/SV/HEX/Topic Modeling/test_set.csv\", sep=\";\")\n",
    "test_set = test_data[\"Volltext\"]  # Texte der Test-Daten\n",
    "ground_truth = test_data[\"Keywords\"]  # Spalte im CSV mit manuell erstellten Begriffen, welche man als korrekt erachtet (i.d.R. einfach wichtige Wörter aus dem Text rauskopieren)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb54a56",
   "metadata": {},
   "source": [
    "## NLP Vorbereitungsschritte\n",
    "Zunächst werden die Trainingsdaten eingelesen und die gängigen Vorbereitungsschritte für NLP durchgeführt. Diese wären:\n",
    "* Stopwords entfernen\n",
    "* CountVectorizer spezifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58fd3e1",
   "metadata": {},
   "source": [
    "### Stopwords entfernen\n",
    "Im Kontext des hier zu modellierenden Topic Modells werden sowohl standardisierte englische, deutsche als auch individuelle Stopwords generiert und im Objekt `sw` zusammengespielt.\n",
    "Die Stopwords können je nach Anwendungsfall ergänzt oder reduziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2c5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_terms = [\n",
    "    \"vl\",\n",
    "    \"übung\",\n",
    "    \"übungen\",\n",
    "    \"seminar\",\n",
    "    \"arbeitsgruppenseminar\",\n",
    "    \"oberseminar\",\n",
    "    \"proseminar\",\n",
    "    \"blockveranstaltung\",\n",
    "    \"vorlesung\",\n",
    "    \"kolloquium\",\n",
    "    \"theoriekolloquium\",\n",
    "    \"einführung\",\n",
    "    \"tutorium\",\n",
    "    \"ue\",\n",
    "    \"vereinbarung\",\n",
    "    \"projekt\",\n",
    "    \"praktikum\",\n",
    "    \"masterprojekt\",\n",
    "    \"wiederholerklausur\",\n",
    "    \"fortgeschrittenenpraktikum\",\n",
    "    \"hauptseminar\",\n",
    "    \"fachpraktikum\",\n",
    "    \"ergänzungsvorlesung\",\n",
    "    \"forschungspraktikum\",\n",
    "    \"begleitseminar\",\n",
    "    \"abschlussarbeiten\",\n",
    "    \"unterrichtspraktikum\",\n",
    "    \"masterseminar\",\n",
    "    \"proseminare\",\n",
    "    \"praxisseminar\",\n",
    "    \"praxissemester\",\n",
    "    \"schulpraxis\",\n",
    "    \"ringpraktikum\",\n",
    "    \"basispraktikum\",\n",
    "    \"praxistage\",\n",
    "    \"industriepraktikum\",\n",
    "    \"vorkurs\",\n",
    "    \"projektseminar\",\n",
    "    \"juniorprofessur\",\n",
    "    \"masterarbeiten\",\n",
    "    \"forschungsseminar\",\n",
    "    \"modulbeschreibung\",\n",
    "    \"veranstaltung\",\n",
    "    \"kommentare\",\n",
    "    \"raum\",\n",
    "    \"uhrzeit\",\n",
    "    \"vereinbarung\",\n",
    "    \"vorlesung\",\n",
    "\n",
    "\n",
    "\n",
    "    # \"Informatik\",\"gleichnamigen\", \"übungsaufgaben\", \"vorlesungsstoff vertieft\", \"vorlesungsstoff\", \"grundlagen programmierung\", \"schriftliche übungsaufgaben\", \"vertieft schriftliche übungsaufgaben\", \"vorlesungsstoff vertieft schriftliche\", \"vertieft schriftliche\", \"lehre\",\n",
    "    # \"siehe\", \"klausur\", \"inf misc\", \"sitzungen\", \"misc\", \"idee\",\n",
    "\n",
    "\n",
    "    # neue university stopwords \n",
    "\"übung\",\n",
    "\"beispiel\",\n",
    "\"studierende\",\n",
    "\"grundlage\",\n",
    "\"laborübung\",\n",
    "\"entwicklung\",\n",
    "\"funktion\",\n",
    "\"praktikum\",\n",
    "\"versuch\",\n",
    "\"methode\",\n",
    "\"modell\",\n",
    "\"kompetente\",\n",
    "\"teil\",\n",
    "\"ziel\",\n",
    "\"schulungsreihe\",\n",
    "\"thema\",\n",
    "\"teilnehmer\",\n",
    "\"bereich\",\n",
    "\"präsentation\",\n",
    "\"einführung\",\n",
    "\"uhr\",\n",
    "\"termin\",\n",
    "\"überblick\",\n",
    "\"aspekt\",\n",
    "\"kontakt\",\n",
    "\"anmeldung\",\n",
    "\"inhalt\",\n",
    "\"kontakt\",\n",
    "\"sprechstunde\",\n",
    "\"anforderung\",\n",
    "\"modell\",\n",
    "\"aufbau\",\n",
    "\"fahren\",\n",
    "\"student\",\n",
    "\"jeweils\",\n",
    "\"studierenden\",\n",
    "\"prof\",\n",
    "\"seminar\",\n",
    "\"seminare\",\n",
    "\"–\",\n",
    "\"fortlaufendes\",\n",
    "\"schwerpunktmodul\",\n",
    "\"entwickelt\",\n",
    "\"handelns\",\n",
    "\"ansatz\",\n",
    "\"schwerpunktmoduls\",\n",
    "\"einzubringen\",\n",
    "\"semestern\",\n",
    "\"semesternseminar\",\n",
    "\"seminarnseminar\",\n",
    "\"zulassung\",\n",
    "\"rahmen\",\n",
    "\"blick\",\n",
    "\"bereit\",\n",
    "\"teilnehmenden\",\n",
    "\"gruppe\",\n",
    "\"teilnehmerinnen\",\n",
    "\"modul\",\n",
    "\"ausdruck\",\n",
    "\"einfluß\",\n",
    "\"kolloquium\",\n",
    "\"thematisiert\",\n",
    "\"bereiche\",\n",
    "\"vermittelt\",\n",
    "\"anhand\",\n",
    "\"schwerpunkt\",\n",
    "\"seminars\",\n",
    "\"lehrperson\",\n",
    "\"deutsche\",\n",
    "\"form\",\n",
    "\"gruppen\",\n",
    "\"gegenstand\",\n",
    "\"anliegen\",\n",
    "\"ansätzen\",\n",
    "\"vorgestellt\",\n",
    "\"ndie\",\n",
    "\"forschungscolleges\",\n",
    "\"erprobt\",\n",
    "\"denkens\",\n",
    "\"ansatzes\",\n",
    "\"vorausgesetzt\",\n",
    "\"montag\",\n",
    "\"studium\",\n",
    "\"blockseminar\",\n",
    "\"veranstaltung\",\n",
    "\"masterstudierende\",\n",
    "\"datum\",\n",
    "\"immatrikulierte\",\n",
    "\"dienstagskolloquium\",\n",
    "\"leistungsnachweise\",\n",
    "\"leistungsnachweis\",\n",
    "\"vgl\",\n",
    "\"the\",\n",
    "\"studiengangs\",\n",
    "\"veranstaltung\",\n",
    "\"seminar\",\n",
    "\"vorlesung\",\n",
    "\"kurs\",\n",
    "\"modul\",\n",
    "\"angebot\",\n",
    "\"behandlung\",\n",
    "\"themen\",\n",
    "\"einführung\",\n",
    "\"grundlagen\",\n",
    "\"aufbau\",\n",
    "\"überblick\",\n",
    "\"inhalt\",\n",
    "\"praxis\",\n",
    "\"theorie\",\n",
    "\"schwerpunkt\",\n",
    "\"recht\",\n",
    "\"rechts\",\n",
    "\"rechtsgebiete\",\n",
    "\"rechtswissenschaft\",\n",
    "\"rechtsfragen\",\n",
    "\"bereich\",\n",
    "\"gebiete\",\n",
    "\"gesetz\",\n",
    "\"juristisch\",\n",
    "\"aktuell\",\n",
    "\"neue\",\n",
    "\"verschiedenen\",\n",
    "\"interdisziplinär\",\n",
    "\"relevant\",\n",
    "\"bedeutung\",\n",
    "\"besonders\",\n",
    "\"werden\",\n",
    "\"sollen\",\n",
    "\"kann\",\n",
    "\"bietet\",\n",
    "\"z.b.\",\n",
    "\"u.a.\",\n",
    "\"etc.\",\n",
    "\"bspw.\",\n",
    "\"ggf.\",\n",
    "\"i.d.r.\",\n",
    "\"family\",\n",
    "\"relationship\",\n",
    "\"practice\",\n",
    "\"lesson\",\n",
    "\"better\",\n",
    "\"lawsuits\",\n",
    "\"they\",\n",
    "\"aid\",\n",
    "\"united\",\n",
    "\"register\",\n",
    "\"between\",\n",
    "\"language\",\n",
    "\"about\",\n",
    "\"einfache\",\n",
    "\"right\",\n",
    "\"acp\",\n",
    "\"before\",\n",
    "\"common\",\n",
    "\"please\",\n",
    "\"matrikel\",\n",
    "\"registration\",\n",
    "\"jurisdictions\",\n",
    "\"across\",\n",
    "\"limits\",\n",
    "\"bundesverfassungsgerichts\",\n",
    "\"künftigen\",\n",
    "\"naturrecht\",\n",
    "\"recent\",\n",
    "\"schriftliches\",\n",
    "\"literaturverarbeitung\",\n",
    "\"kurzvortrag\",\n",
    "\"genügt\",\n",
    "\"seelmann\",\n",
    "\"referats\",\n",
    "\"lage\",\n",
    "\"fighting\",\n",
    "\"neuster\",\n",
    "\"arbeitsmethoden\",\n",
    "\"eugh\",\n",
    "\"algorithmus\",\n",
    "\"formel\",\n",
    "\"adresse\",\n",
    "\"resolution\",\n",
    "\"dispute\",\n",
    "\"module\",\n",
    "\"literaturhinweise\",\n",
    "\"civil\",\n",
    "\"regulatory\",\n",
    "\"menschenwürde\",\n",
    "\"machthabern\",\n",
    "\n",
    "\n",
    "    # solariz stopwords \n",
    "    \"ab\",\n",
    "  \"aber\",\n",
    "  \"abermaliges\",\n",
    "  \"abermals\",\n",
    "  \"abgerufen\",\n",
    "  \"abgerufene\",\n",
    "  \"abgerufener\",\n",
    "  \"abgerufenes\",\n",
    "  \"abgesehen\",\n",
    "  \"acht\",\n",
    "  \"aehnlich\",\n",
    "  \"aehnliche\",\n",
    "  \"aehnlichem\",\n",
    "  \"aehnlichen\",\n",
    "  \"aehnlicher\",\n",
    "  \"aehnliches\",\n",
    "  \"aehnlichste\",\n",
    "  \"aehnlichstem\",\n",
    "  \"aehnlichsten\",\n",
    "  \"aehnlichster\",\n",
    "  \"aehnlichstes\",\n",
    "  \"aeusserst\",\n",
    "  \"aeusserste\",\n",
    "  \"aeusserstem\",\n",
    "  \"aeussersten\",\n",
    "  \"aeusserster\",\n",
    "  \"aeusserstes\",\n",
    "  \"ähnlich\",\n",
    "  \"ähnliche\",\n",
    "  \"ähnlichem\",\n",
    "  \"ähnlichen\",\n",
    "  \"ähnlicher\",\n",
    "  \"ähnliches\",\n",
    "  \"ähnlichst\",\n",
    "  \"ähnlichste\",\n",
    "  \"ähnlichstem\",\n",
    "  \"ähnlichsten\",\n",
    "  \"ähnlichster\",\n",
    "  \"ähnlichstes\",\n",
    "  \"alle\",\n",
    "  \"allein\",\n",
    "  \"alleine\",\n",
    "  \"allem\",\n",
    "  \"allemal\",\n",
    "  \"allen\",\n",
    "  \"allenfalls\",\n",
    "  \"allenthalben\",\n",
    "  \"aller\",\n",
    "  \"allerdings\",\n",
    "  \"allerlei\",\n",
    "  \"alles\",\n",
    "  \"allesamt\",\n",
    "  \"allg\",\n",
    "  \"allg.\",\n",
    "  \"allgemein\",\n",
    "  \"allgemeine\",\n",
    "  \"allgemeinem\",\n",
    "  \"allgemeinen\",\n",
    "  \"allgemeiner\",\n",
    "  \"allgemeines\",\n",
    "  \"allgemeinste\",\n",
    "  \"allgemeinstem\",\n",
    "  \"allgemeinsten\",\n",
    "  \"allgemeinster\",\n",
    "  \"allgemeinstes\",\n",
    "  \"allmählich\",\n",
    "  \"allzeit\",\n",
    "  \"allzu\",\n",
    "  \"als\",\n",
    "  \"alsbald\",\n",
    "  \"also\",\n",
    "  \"am\",\n",
    "  \"an\",\n",
    "  \"and\",\n",
    "  \"andauernd\",\n",
    "  \"andauernde\",\n",
    "  \"andauerndem\",\n",
    "  \"andauernden\",\n",
    "  \"andauernder\",\n",
    "  \"andauerndes\",\n",
    "  \"ander\",\n",
    "  \"andere\",\n",
    "  \"anderem\",\n",
    "  \"anderen\",\n",
    "  \"anderenfalls\",\n",
    "  \"anderer\",\n",
    "  \"andererseits\",\n",
    "  \"anderes\",\n",
    "  \"anderm\",\n",
    "  \"andern\",\n",
    "  \"andernfalls\",\n",
    "  \"anderr\",\n",
    "  \"anders\",\n",
    "  \"anderst\",\n",
    "  \"anderweitig\",\n",
    "  \"anderweitige\",\n",
    "  \"anderweitigem\",\n",
    "  \"anderweitigen\",\n",
    "  \"anderweitiger\",\n",
    "  \"anderweitiges\",\n",
    "  \"anerkannt\",\n",
    "  \"anerkannte\",\n",
    "  \"anerkannter\",\n",
    "  \"anerkanntes\",\n",
    "  \"anfangen\",\n",
    "  \"anfing\",\n",
    "  \"angefangen\",\n",
    "  \"angesetze\",\n",
    "  \"angesetzt\",\n",
    "  \"angesetzten\",\n",
    "  \"angesetzter\",\n",
    "  \"ans\",\n",
    "  \"anscheinend\",\n",
    "  \"ansetzen\",\n",
    "  \"ansonst\",\n",
    "  \"ansonsten\",\n",
    "  \"anstatt\",\n",
    "  \"anstelle\",\n",
    "  \"arbeiten\",\n",
    "  \"auch\",\n",
    "  \"auf\",\n",
    "  \"aufgehört\",\n",
    "  \"aufgrund\",\n",
    "  \"aufhören\",\n",
    "  \"aufhörte\",\n",
    "  \"aufzusuchen\",\n",
    "  \"augenscheinlich\",\n",
    "  \"augenscheinliche\",\n",
    "  \"augenscheinlichem\",\n",
    "  \"augenscheinlichen\",\n",
    "  \"augenscheinlicher\",\n",
    "  \"augenscheinliches\",\n",
    "  \"augenscheinlichst\",\n",
    "  \"augenscheinlichste\",\n",
    "  \"augenscheinlichstem\",\n",
    "  \"augenscheinlichsten\",\n",
    "  \"augenscheinlichster\",\n",
    "  \"augenscheinlichstes\",\n",
    "  \"aus\",\n",
    "  \"ausdrücken\",\n",
    "  \"ausdrücklich\",\n",
    "  \"ausdrückliche\",\n",
    "  \"ausdrücklichem\",\n",
    "  \"ausdrücklichen\",\n",
    "  \"ausdrücklicher\",\n",
    "  \"ausdrückliches\",\n",
    "  \"ausdrückt\",\n",
    "  \"ausdrückte\",\n",
    "  \"ausgenommen\",\n",
    "  \"ausgenommene\",\n",
    "  \"ausgenommenem\",\n",
    "  \"ausgenommenen\",\n",
    "  \"ausgenommener\",\n",
    "  \"ausgenommenes\",\n",
    "  \"ausgerechnet\",\n",
    "  \"ausgerechnete\",\n",
    "  \"ausgerechnetem\",\n",
    "  \"ausgerechneten\",\n",
    "  \"ausgerechneter\",\n",
    "  \"ausgerechnetes\",\n",
    "  \"ausnahmslos\",\n",
    "  \"ausnahmslose\",\n",
    "  \"ausnahmslosem\",\n",
    "  \"ausnahmslosen\",\n",
    "  \"ausnahmsloser\",\n",
    "  \"ausnahmsloses\",\n",
    "  \"außen\",\n",
    "  \"ausser\",\n",
    "  \"ausserdem\",\n",
    "  \"außerhalb\",\n",
    "  \"äusserst\",\n",
    "  \"äusserste\",\n",
    "  \"äusserstem\",\n",
    "  \"äussersten\",\n",
    "  \"äusserster\",\n",
    "  \"äusserstes\",\n",
    "  \"author\",\n",
    "  \"autor\",\n",
    "  \"baelde\",\n",
    "  \"bald\",\n",
    "  \"bälde\",\n",
    "  \"bearbeite\",\n",
    "  \"bearbeiten\",\n",
    "  \"bearbeitete\",\n",
    "  \"bearbeiteten\",\n",
    "  \"bedarf\",\n",
    "  \"bedürfen\",\n",
    "  \"bedurfte\",\n",
    "  \"been\",\n",
    "  \"befahl\",\n",
    "  \"befiehlt\",\n",
    "  \"befiehlte\",\n",
    "  \"befohlene\",\n",
    "  \"befohlens\",\n",
    "  \"befragen\",\n",
    "  \"befragte\",\n",
    "  \"befragten\",\n",
    "  \"befragter\",\n",
    "  \"begann\",\n",
    "  \"beginnen\",\n",
    "  \"begonnen\",\n",
    "  \"behalten\",\n",
    "  \"behielt\",\n",
    "  \"bei\",\n",
    "  \"beide\",\n",
    "  \"beidem\",\n",
    "  \"beiden\",\n",
    "  \"beider\",\n",
    "  \"beiderlei\",\n",
    "  \"beides\",\n",
    "  \"beim\",\n",
    "  \"beinahe\",\n",
    "  \"beisammen\",\n",
    "  \"beispielsweise\",\n",
    "  \"beitragen\",\n",
    "  \"beitrugen\",\n",
    "  \"bekannt\",\n",
    "  \"bekannte\",\n",
    "  \"bekannter\",\n",
    "  \"bekanntlich\",\n",
    "  \"bekanntliche\",\n",
    "  \"bekanntlichem\",\n",
    "  \"bekanntlichen\",\n",
    "  \"bekanntlicher\",\n",
    "  \"bekanntliches\",\n",
    "  \"bekennen\",\n",
    "  \"benutzt\",\n",
    "  \"bereits\",\n",
    "  \"berichten\",\n",
    "  \"berichtet\",\n",
    "  \"berichtete\",\n",
    "  \"berichteten\",\n",
    "  \"besonders\",\n",
    "  \"besser\",\n",
    "  \"bessere\",\n",
    "  \"besserem\",\n",
    "  \"besseren\",\n",
    "  \"besserer\",\n",
    "  \"besseres\",\n",
    "  \"bestehen\",\n",
    "  \"besteht\",\n",
    "  \"bestenfalls\",\n",
    "  \"bestimmt\",\n",
    "  \"bestimmte\",\n",
    "  \"bestimmtem\",\n",
    "  \"bestimmten\",\n",
    "  \"bestimmter\",\n",
    "  \"bestimmtes\",\n",
    "  \"beträchtlich\",\n",
    "  \"beträchtliche\",\n",
    "  \"beträchtlichem\",\n",
    "  \"beträchtlichen\",\n",
    "  \"beträchtlicher\",\n",
    "  \"beträchtliches\",\n",
    "  \"betraechtlich\",\n",
    "  \"betraechtliche\",\n",
    "  \"betraechtlichem\",\n",
    "  \"betraechtlichen\",\n",
    "  \"betraechtlicher\",\n",
    "  \"betraechtliches\",\n",
    "  \"betreffend\",\n",
    "  \"betreffende\",\n",
    "  \"betreffendem\",\n",
    "  \"betreffenden\",\n",
    "  \"betreffender\",\n",
    "  \"betreffendes\",\n",
    "  \"bevor\",\n",
    "  \"bez\",\n",
    "  \"bez.\",\n",
    "  \"bezgl\",\n",
    "  \"bezgl.\",\n",
    "  \"bezueglich\",\n",
    "  \"bezüglich\",\n",
    "  \"bietet\",\n",
    "  \"bin\",\n",
    "  \"bis\",\n",
    "  \"bisher\",\n",
    "  \"bisherige\",\n",
    "  \"bisherigem\",\n",
    "  \"bisherigen\",\n",
    "  \"bisheriger\",\n",
    "  \"bisheriges\",\n",
    "  \"bislang\",\n",
    "  \"bisschen\",\n",
    "  \"bist\",\n",
    "  \"bitte\",\n",
    "  \"bleiben\",\n",
    "  \"bleibt\",\n",
    "  \"blieb\",\n",
    "  \"bloss\",\n",
    "  \"böden\",\n",
    "  \"boeden\",\n",
    "  \"brachte\",\n",
    "  \"brachten\",\n",
    "  \"brauchen\",\n",
    "  \"braucht\",\n",
    "  \"bräuchte\",\n",
    "  \"bringen\",\n",
    "  \"bsp\",\n",
    "  \"bsp.\",\n",
    "  \"bspw\",\n",
    "  \"bspw.\",\n",
    "  \"bzw\",\n",
    "  \"bzw.\",\n",
    "  \"ca\",\n",
    "  \"ca.\",\n",
    "  \"circa\",\n",
    "  \"da\",\n",
    "  \"dabei\",\n",
    "  \"dadurch\",\n",
    "  \"dafuer\",\n",
    "  \"dafür\",\n",
    "  \"dagegen\",\n",
    "  \"daher\",\n",
    "  \"dahin\",\n",
    "  \"dahingehend\",\n",
    "  \"dahingehende\",\n",
    "  \"dahingehendem\",\n",
    "  \"dahingehenden\",\n",
    "  \"dahingehender\",\n",
    "  \"dahingehendes\",\n",
    "  \"dahinter\",\n",
    "  \"damalige\",\n",
    "  \"damaligem\",\n",
    "  \"damaligen\",\n",
    "  \"damaliger\",\n",
    "  \"damaliges\",\n",
    "  \"damals\",\n",
    "  \"damit\",\n",
    "  \"danach\",\n",
    "  \"daneben\",\n",
    "  \"dank\",\n",
    "  \"danke\",\n",
    "  \"danken\",\n",
    "  \"dann\",\n",
    "  \"dannen\",\n",
    "  \"daran\",\n",
    "  \"darauf\",\n",
    "  \"daraus\",\n",
    "  \"darf\",\n",
    "  \"darfst\",\n",
    "  \"darin\",\n",
    "  \"darüber\",\n",
    "  \"darüberhinaus\",\n",
    "  \"darueber\",\n",
    "  \"darueberhinaus\",\n",
    "  \"darum\",\n",
    "  \"darunter\",\n",
    "  \"das\",\n",
    "  \"daß\",\n",
    "  \"dass\",\n",
    "  \"dasselbe\",\n",
    "  \"Dat\",\n",
    "  \"davon\",\n",
    "  \"davor\",\n",
    "  \"dazu\",\n",
    "  \"dazwischen\",\n",
    "  \"dein\",\n",
    "  \"deine\",\n",
    "  \"deinem\",\n",
    "  \"deinen\",\n",
    "  \"deiner\",\n",
    "  \"deines\",\n",
    "  \"dem\",\n",
    "  \"demgegenüber\",\n",
    "  \"demgegenueber\",\n",
    "  \"demgemaess\",\n",
    "  \"demgemäss\",\n",
    "  \"demnach\",\n",
    "  \"demselben\",\n",
    "  \"den\",\n",
    "  \"denen\",\n",
    "  \"denkbar\",\n",
    "  \"denkbare\",\n",
    "  \"denkbarem\",\n",
    "  \"denkbaren\",\n",
    "  \"denkbarer\",\n",
    "  \"denkbares\",\n",
    "  \"denn\",\n",
    "  \"dennoch\",\n",
    "  \"denselben\",\n",
    "  \"der\",\n",
    "  \"derart\",\n",
    "  \"derartig\",\n",
    "  \"derartige\",\n",
    "  \"derartigem\",\n",
    "  \"derartigen\",\n",
    "  \"derartiger\",\n",
    "  \"derem\",\n",
    "  \"deren\",\n",
    "  \"derer\",\n",
    "  \"derjenige\",\n",
    "  \"derjenigen\",\n",
    "  \"derselbe\",\n",
    "  \"derselben\",\n",
    "  \"derzeit\",\n",
    "  \"derzeitig\",\n",
    "  \"derzeitige\",\n",
    "  \"derzeitigem\",\n",
    "  \"derzeitigen\",\n",
    "  \"derzeitiges\",\n",
    "  \"des\",\n",
    "  \"deshalb\",\n",
    "  \"desselben\",\n",
    "  \"dessen\",\n",
    "  \"dessenungeachtet\",\n",
    "  \"desto\",\n",
    "  \"desungeachtet\",\n",
    "  \"deswegen\",\n",
    "  \"dich\",\n",
    "  \"die\",\n",
    "  \"diejenige\",\n",
    "  \"diejenigen\",\n",
    "  \"dies\",\n",
    "  \"diese\",\n",
    "  \"dieselbe\",\n",
    "  \"dieselben\",\n",
    "  \"diesem\",\n",
    "  \"diesen\",\n",
    "  \"dieser\",\n",
    "  \"dieses\",\n",
    "  \"diesseitig\",\n",
    "  \"diesseitige\",\n",
    "  \"diesseitigem\",\n",
    "  \"diesseitigen\",\n",
    "  \"diesseitiger\",\n",
    "  \"diesseitiges\",\n",
    "  \"diesseits\",\n",
    "  \"dinge\",\n",
    "  \"dir\",\n",
    "  \"direkt\",\n",
    "  \"direkte\",\n",
    "  \"direkten\",\n",
    "  \"direkter\",\n",
    "  \"doch\",\n",
    "  \"doppelt\",\n",
    "  \"dort\",\n",
    "  \"dorther\",\n",
    "  \"dorthin\",\n",
    "  \"dran\",\n",
    "  \"drauf\",\n",
    "  \"drei\",\n",
    "  \"dreißig\",\n",
    "  \"drin\",\n",
    "  \"dritte\",\n",
    "  \"drüber\",\n",
    "  \"drueber\",\n",
    "  \"drum\",\n",
    "  \"drunter\",\n",
    "  \"du\",\n",
    "  \"duerfte\",\n",
    "  \"duerften\",\n",
    "  \"duerftest\",\n",
    "  \"duerftet\",\n",
    "  \"dunklen\",\n",
    "  \"durch\",\n",
    "  \"durchaus\",\n",
    "  \"durchweg\",\n",
    "  \"durchwegs\",\n",
    "  \"dürfen\",\n",
    "  \"durfte\",\n",
    "  \"dürfte\",\n",
    "  \"durften\",\n",
    "  \"dürften\",\n",
    "  \"durftest\",\n",
    "  \"dürftest\",\n",
    "  \"durftet\",\n",
    "  \"dürftet\",\n",
    "  \"eben\",\n",
    "  \"ebenfalls\",\n",
    "  \"ebenso\",\n",
    "  \"ect\",\n",
    "  \"ect.\",\n",
    "  \"ehe\",\n",
    "  \"eher\",\n",
    "  \"eheste\",\n",
    "  \"ehestem\",\n",
    "  \"ehesten\",\n",
    "  \"ehester\",\n",
    "  \"ehestes\",\n",
    "  \"eigen\",\n",
    "  \"eigene\",\n",
    "  \"eigenem\",\n",
    "  \"eigenen\",\n",
    "  \"eigener\",\n",
    "  \"eigenes\",\n",
    "  \"eigenst\",\n",
    "  \"eigentlich\",\n",
    "  \"eigentliche\",\n",
    "  \"eigentlichem\",\n",
    "  \"eigentlichen\",\n",
    "  \"eigentlicher\",\n",
    "  \"eigentliches\",\n",
    "  \"ein\",\n",
    "  \"einbaün\",\n",
    "  \"eine\",\n",
    "  \"einem\",\n",
    "  \"einen\",\n",
    "  \"einer\",\n",
    "  \"einerlei\",\n",
    "  \"einerseits\",\n",
    "  \"eines\",\n",
    "  \"einfach\",\n",
    "  \"einführen\",\n",
    "  \"einführte\",\n",
    "  \"einführten\",\n",
    "  \"eingesetzt\",\n",
    "  \"einig\",\n",
    "  \"einige\",\n",
    "  \"einigem\",\n",
    "  \"einigen\",\n",
    "  \"einiger\",\n",
    "  \"einigermaßen\",\n",
    "  \"einiges\",\n",
    "  \"einmal\",\n",
    "  \"einmalig\",\n",
    "  \"einmalige\",\n",
    "  \"einmaligem\",\n",
    "  \"einmaligen\",\n",
    "  \"einmaliger\",\n",
    "  \"einmaliges\",\n",
    "  \"eins\",\n",
    "  \"einseitig\",\n",
    "  \"einseitige\",\n",
    "  \"einseitigen\",\n",
    "  \"einseitiger\",\n",
    "  \"einst\",\n",
    "  \"einstmals\",\n",
    "  \"einzig\",\n",
    "  \"empfunden\",\n",
    "  \"ende\",\n",
    "  \"entgegen\",\n",
    "  \"entlang\",\n",
    "  \"entsprechend\",\n",
    "  \"entsprechende\",\n",
    "  \"entsprechendem\",\n",
    "  \"entsprechenden\",\n",
    "  \"entsprechender\",\n",
    "  \"entsprechendes\",\n",
    "  \"entweder\",\n",
    "  \"er\",\n",
    "  \"ergänze\",\n",
    "  \"ergänzen\",\n",
    "  \"ergänzte\",\n",
    "  \"ergänzten\",\n",
    "  \"ergo\",\n",
    "  \"erhält\",\n",
    "  \"erhalten\",\n",
    "  \"erhielt\",\n",
    "  \"erhielten\",\n",
    "  \"erneut\",\n",
    "  \"eröffne\",\n",
    "  \"eröffnen\",\n",
    "  \"eröffnet\",\n",
    "  \"eröffnete\",\n",
    "  \"eröffnetes\",\n",
    "  \"erscheinen\",\n",
    "  \"erst\",\n",
    "  \"erste\",\n",
    "  \"erstem\",\n",
    "  \"ersten\",\n",
    "  \"erster\",\n",
    "  \"erstere\",\n",
    "  \"ersterem\",\n",
    "  \"ersteren\",\n",
    "  \"ersterer\",\n",
    "  \"ersteres\",\n",
    "  \"erstes\",\n",
    "  \"es\",\n",
    "  \"etc\",\n",
    "  \"etc.\",\n",
    "  \"etliche\",\n",
    "  \"etlichem\",\n",
    "  \"etlichen\",\n",
    "  \"etlicher\",\n",
    "  \"etliches\",\n",
    "  \"etwa\",\n",
    "  \"etwaige\",\n",
    "  \"etwas\",\n",
    "  \"euch\",\n",
    "  \"euer\",\n",
    "  \"eure\",\n",
    "  \"eurem\",\n",
    "  \"euren\",\n",
    "  \"eurer\",\n",
    "  \"eures\",\n",
    "  \"euretwegen\",\n",
    "  \"fall\",\n",
    "  \"falls\",\n",
    "  \"fand\",\n",
    "  \"fast\",\n",
    "  \"ferner\",\n",
    "  \"fertig\",\n",
    "  \"finde\",\n",
    "  \"finden\",\n",
    "  \"findest\",\n",
    "  \"findet\",\n",
    "  \"folgend\",\n",
    "  \"folgende\",\n",
    "  \"folgendem\",\n",
    "  \"folgenden\",\n",
    "  \"folgender\",\n",
    "  \"folgendermassen\",\n",
    "  \"folgendes\",\n",
    "  \"folglich\",\n",
    "  \"for\",\n",
    "  \"fordern\",\n",
    "  \"fordert\",\n",
    "  \"forderte\",\n",
    "  \"forderten\",\n",
    "  \"fort\",\n",
    "  \"fortsetzen\",\n",
    "  \"fortsetzt\",\n",
    "  \"fortsetzte\",\n",
    "  \"fortsetzten\",\n",
    "  \"fragte\",\n",
    "  \"frau\",\n",
    "  \"frei\",\n",
    "  \"freie\",\n",
    "  \"freier\",\n",
    "  \"freies\",\n",
    "  \"fuer\",\n",
    "  \"fuers\",\n",
    "  \"fünf\",\n",
    "  \"für\",\n",
    "  \"fürs\",\n",
    "  \"gab\",\n",
    "  \"gaenzlich\",\n",
    "  \"gaenzliche\",\n",
    "  \"gaenzlichem\",\n",
    "  \"gaenzlichen\",\n",
    "  \"gaenzlicher\",\n",
    "  \"gaenzliches\",\n",
    "  \"gängig\",\n",
    "  \"gängige\",\n",
    "  \"gängigen\",\n",
    "  \"gängiger\",\n",
    "  \"gängiges\",\n",
    "  \"ganz\",\n",
    "  \"ganze\",\n",
    "  \"ganzem\",\n",
    "  \"ganzen\",\n",
    "  \"ganzer\",\n",
    "  \"ganzes\",\n",
    "  \"gänzlich\",\n",
    "  \"gänzliche\",\n",
    "  \"gänzlichem\",\n",
    "  \"gänzlichen\",\n",
    "  \"gänzlicher\",\n",
    "  \"gänzliches\",\n",
    "  \"gar\",\n",
    "  \"gbr\",\n",
    "  \"geb\",\n",
    "  \"geben\",\n",
    "  \"geblieben\",\n",
    "  \"gebracht\",\n",
    "  \"gedurft\",\n",
    "  \"geehrt\",\n",
    "  \"geehrte\",\n",
    "  \"geehrten\",\n",
    "  \"geehrter\",\n",
    "  \"gefallen\",\n",
    "  \"gefälligst\",\n",
    "  \"gefällt\",\n",
    "  \"gefiel\",\n",
    "  \"gegeben\",\n",
    "  \"gegen\",\n",
    "  \"gegenüber\",\n",
    "  \"gegenueber\",\n",
    "  \"gehabt\",\n",
    "  \"gehalten\",\n",
    "  \"gehen\",\n",
    "  \"geht\",\n",
    "  \"gekommen\",\n",
    "  \"gekonnt\",\n",
    "  \"gemacht\",\n",
    "  \"gemaess\",\n",
    "  \"gemäss\",\n",
    "  \"gemeinhin\",\n",
    "  \"gemocht\",\n",
    "  \"genau\",\n",
    "  \"genommen\",\n",
    "  \"genug\",\n",
    "  \"gepriesener\",\n",
    "  \"gepriesenes\",\n",
    "  \"gerade\",\n",
    "  \"gern\",\n",
    "  \"gesagt\",\n",
    "  \"gesehen\",\n",
    "  \"gestern\",\n",
    "  \"gestrige\",\n",
    "  \"getan\",\n",
    "  \"geteilt\",\n",
    "  \"geteilte\",\n",
    "  \"getragen\",\n",
    "  \"getrennt\",\n",
    "  \"gewesen\",\n",
    "  \"gewiss\",\n",
    "  \"gewisse\",\n",
    "  \"gewissem\",\n",
    "  \"gewissen\",\n",
    "  \"gewisser\",\n",
    "  \"gewissermaßen\",\n",
    "  \"gewisses\",\n",
    "  \"gewollt\",\n",
    "  \"geworden\",\n",
    "  \"ggf\",\n",
    "  \"ggf.\",\n",
    "  \"gib\",\n",
    "  \"gibt\",\n",
    "  \"gilt\",\n",
    "  \"gleich\",\n",
    "  \"gleiche\",\n",
    "  \"gleichem\",\n",
    "  \"gleichen\",\n",
    "  \"gleicher\",\n",
    "  \"gleiches\",\n",
    "  \"gleichsam\",\n",
    "  \"gleichste\",\n",
    "  \"gleichstem\",\n",
    "  \"gleichsten\",\n",
    "  \"gleichster\",\n",
    "  \"gleichstes\",\n",
    "  \"gleichwohl\",\n",
    "  \"gleichzeitig\",\n",
    "  \"gleichzeitige\",\n",
    "  \"gleichzeitigem\",\n",
    "  \"gleichzeitigen\",\n",
    "  \"gleichzeitiger\",\n",
    "  \"gleichzeitiges\",\n",
    "  \"glücklicherweise\",\n",
    "  \"gluecklicherweise\",\n",
    "  \"gmbh\",\n",
    "  \"gottseidank\",\n",
    "  \"gratulieren\",\n",
    "  \"gratuliert\",\n",
    "  \"gratulierte\",\n",
    "  \"groesstenteils\",\n",
    "  \"grösstenteils\",\n",
    "  \"gruendlich\",\n",
    "  \"gründlich\",\n",
    "  \"gut\",\n",
    "  \"gute\",\n",
    "  \"guten\",\n",
    "  \"hab\",\n",
    "  \"habe\",\n",
    "  \"haben\",\n",
    "  \"habt\",\n",
    "  \"haette\",\n",
    "  \"haeufig\",\n",
    "  \"haeufige\",\n",
    "  \"haeufigem\",\n",
    "  \"haeufigen\",\n",
    "  \"haeufiger\",\n",
    "  \"haeufigere\",\n",
    "  \"haeufigeren\",\n",
    "  \"haeufigerer\",\n",
    "  \"haeufigeres\",\n",
    "  \"halb\",\n",
    "  \"hallo\",\n",
    "  \"halten\",\n",
    "  \"hast\",\n",
    "  \"hat\",\n",
    "  \"hätt\",\n",
    "  \"hatte\",\n",
    "  \"hätte\",\n",
    "  \"hatten\",\n",
    "  \"hätten\",\n",
    "  \"hattest\",\n",
    "  \"hattet\",\n",
    "  \"häufig\",\n",
    "  \"häufige\",\n",
    "  \"häufigem\",\n",
    "  \"häufigen\",\n",
    "  \"häufiger\",\n",
    "  \"häufigere\",\n",
    "  \"häufigeren\",\n",
    "  \"häufigerer\",\n",
    "  \"häufigeres\",\n",
    "  \"hen\",\n",
    "  \"her\",\n",
    "  \"heraus\",\n",
    "  \"herein\",\n",
    "  \"herum\",\n",
    "  \"heute\",\n",
    "  \"heutige\",\n",
    "  \"heutigem\",\n",
    "  \"heutigen\",\n",
    "  \"heutiger\",\n",
    "  \"heutiges\",\n",
    "  \"hier\",\n",
    "  \"hierbei\",\n",
    "  \"hiermit\",\n",
    "  \"hiesige\",\n",
    "  \"hiesigem\",\n",
    "  \"hiesigen\",\n",
    "  \"hiesiger\",\n",
    "  \"hiesiges\",\n",
    "  \"hin\",\n",
    "  \"hindurch\",\n",
    "  \"hinein\",\n",
    "  \"hingegen\",\n",
    "  \"hinlanglich\",\n",
    "  \"hinlänglich\",\n",
    "  \"hinten\",\n",
    "  \"hintendran\",\n",
    "  \"hinter\",\n",
    "  \"hinterher\",\n",
    "  \"hinterm\",\n",
    "  \"hintern\",\n",
    "  \"hinunter\",\n",
    "  \"hoch\",\n",
    "  \"höchst\",\n",
    "  \"höchstens\",\n",
    "  \"http\",\n",
    "  \"hundert\",\n",
    "  \"ich\",\n",
    "  \"igitt\",\n",
    "  \"ihm\",\n",
    "  \"ihn\",\n",
    "  \"ihnen\",\n",
    "  \"ihr\",\n",
    "  \"ihre\",\n",
    "  \"ihrem\",\n",
    "  \"ihren\",\n",
    "  \"ihrer\",\n",
    "  \"ihres\",\n",
    "  \"ihretwegen\",\n",
    "  \"ihrige\",\n",
    "  \"ihrigen\",\n",
    "  \"ihriges\",\n",
    "  \"im\",\n",
    "  \"immer\",\n",
    "  \"immerhin\",\n",
    "  \"immerwaehrend\",\n",
    "  \"immerwaehrende\",\n",
    "  \"immerwaehrendem\",\n",
    "  \"immerwaehrenden\",\n",
    "  \"immerwaehrender\",\n",
    "  \"immerwaehrendes\",\n",
    "  \"immerwährend\",\n",
    "  \"immerwährende\",\n",
    "  \"immerwährendem\",\n",
    "  \"immerwährenden\",\n",
    "  \"immerwährender\",\n",
    "  \"immerwährendes\",\n",
    "  \"immerzu\",\n",
    "  \"important\",\n",
    "  \"in\",\n",
    "  \"indem\",\n",
    "  \"indessen\",\n",
    "  \"Inf.\",\n",
    "  \"info\",\n",
    "  \"infolge\",\n",
    "  \"infolgedessen\",\n",
    "  \"information\",\n",
    "  \"innen\",\n",
    "  \"innerhalb\",\n",
    "  \"innerlich\",\n",
    "  \"ins\",\n",
    "  \"insbesondere\",\n",
    "  \"insgeheim\",\n",
    "  \"insgeheime\",\n",
    "  \"insgeheimer\",\n",
    "  \"insgesamt\",\n",
    "  \"insgesamte\",\n",
    "  \"insgesamter\",\n",
    "  \"insofern\",\n",
    "  \"inzwischen\",\n",
    "  \"irgend\",\n",
    "  \"irgendein\",\n",
    "  \"irgendeine\",\n",
    "  \"irgendeinem\",\n",
    "  \"irgendeiner\",\n",
    "  \"irgendeines\",\n",
    "  \"irgendetwas\",\n",
    "  \"irgendjemand\",\n",
    "  \"irgendjemandem\",\n",
    "  \"irgendwann\",\n",
    "  \"irgendwas\",\n",
    "  \"irgendwelche\",\n",
    "  \"irgendwen\",\n",
    "  \"irgendwenn\",\n",
    "  \"irgendwer\",\n",
    "  \"irgendwie\",\n",
    "  \"irgendwo\",\n",
    "  \"irgendwohin\",\n",
    "  \"ist\",\n",
    "  \"ja\",\n",
    "  \"jaehrig\",\n",
    "  \"jaehrige\",\n",
    "  \"jaehrigem\",\n",
    "  \"jaehrigen\",\n",
    "  \"jaehriger\",\n",
    "  \"jaehriges\",\n",
    "  \"jährig\",\n",
    "  \"jährige\",\n",
    "  \"jährigem\",\n",
    "  \"jährigen\",\n",
    "  \"jähriges\",\n",
    "  \"je\",\n",
    "  \"jede\",\n",
    "  \"jedem\",\n",
    "  \"jeden\",\n",
    "  \"jedenfalls\",\n",
    "  \"jeder\",\n",
    "  \"jederlei\",\n",
    "  \"jedes\",\n",
    "  \"jedesmal\",\n",
    "  \"jedoch\",\n",
    "  \"jeglichem\",\n",
    "  \"jeglichen\",\n",
    "  \"jeglicher\",\n",
    "  \"jegliches\",\n",
    "  \"jemals\",\n",
    "  \"jemand\",\n",
    "  \"jemandem\",\n",
    "  \"jemanden\",\n",
    "  \"jemandes\",\n",
    "  \"jene\",\n",
    "  \"jenem\",\n",
    "  \"jenen\",\n",
    "  \"jener\",\n",
    "  \"jenes\",\n",
    "  \"jenseitig\",\n",
    "  \"jenseitigem\",\n",
    "  \"jenseitiger\",\n",
    "  \"jenseits\",\n",
    "  \"jetzt\",\n",
    "  \"jung\",\n",
    "  \"junge\",\n",
    "  \"jungem\",\n",
    "  \"jungen\",\n",
    "  \"junger\",\n",
    "  \"junges\",\n",
    "  \"kaeumlich\",\n",
    "  \"kam\",\n",
    "  \"kann\",\n",
    "  \"kannst\",\n",
    "  \"kaum\",\n",
    "  \"käumlich\",\n",
    "  \"kein\",\n",
    "  \"keine\",\n",
    "  \"keinem\",\n",
    "  \"keinen\",\n",
    "  \"keiner\",\n",
    "  \"keinerlei\",\n",
    "  \"keines\",\n",
    "  \"keineswegs\",\n",
    "  \"klar\",\n",
    "  \"klare\",\n",
    "  \"klaren\",\n",
    "  \"klares\",\n",
    "  \"klein\",\n",
    "  \"kleinen\",\n",
    "  \"kleiner\",\n",
    "  \"kleines\",\n",
    "  \"koennen\",\n",
    "  \"koennt\",\n",
    "  \"koennte\",\n",
    "  \"koennten\",\n",
    "  \"koenntest\",\n",
    "  \"koenntet\",\n",
    "  \"komme\",\n",
    "  \"kommen\",\n",
    "  \"kommt\",\n",
    "  \"konkret\",\n",
    "  \"konkrete\",\n",
    "  \"konkreten\",\n",
    "  \"konkreter\",\n",
    "  \"konkretes\",\n",
    "  \"könn\",\n",
    "  \"können\",\n",
    "  \"könnt\",\n",
    "  \"konnte\",\n",
    "  \"könnte\",\n",
    "  \"konnten\",\n",
    "  \"könnten\",\n",
    "  \"konntest\",\n",
    "  \"könntest\",\n",
    "  \"konntet\",\n",
    "  \"könntet\",\n",
    "  \"kuenftig\",\n",
    "  \"kuerzlich\",\n",
    "  \"kuerzlichst\",\n",
    "  \"künftig\",\n",
    "  \"kürzlich\",\n",
    "  \"kürzlichst\",\n",
    "  \"laengst\",\n",
    "  \"lag\",\n",
    "  \"lagen\",\n",
    "  \"langsam\",\n",
    "  \"längst\",\n",
    "  \"längstens\",\n",
    "  \"lassen\",\n",
    "  \"laut\",\n",
    "  \"lediglich\",\n",
    "  \"leer\",\n",
    "  \"legen\",\n",
    "  \"legte\",\n",
    "  \"legten\",\n",
    "  \"leicht\",\n",
    "  \"leider\",\n",
    "  \"lesen\",\n",
    "  \"letze\",\n",
    "  \"letzte\",\n",
    "  \"letzten\",\n",
    "  \"letztendlich\",\n",
    "  \"letztens\",\n",
    "  \"letztere\",\n",
    "  \"letzterem\",\n",
    "  \"letzterer\",\n",
    "  \"letzteres\",\n",
    "  \"letztes\",\n",
    "  \"letztlich\",\n",
    "  \"lichten\",\n",
    "  \"liegt\",\n",
    "  \"liest\",\n",
    "  \"links\",\n",
    "  \"mache\",\n",
    "  \"machen\",\n",
    "  \"machst\",\n",
    "  \"macht\",\n",
    "  \"machte\",\n",
    "  \"machten\",\n",
    "  \"mag\",\n",
    "  \"magst\",\n",
    "  \"mal\",\n",
    "  \"man\",\n",
    "  \"manch\",\n",
    "  \"manche\",\n",
    "  \"manchem\",\n",
    "  \"manchen\",\n",
    "  \"mancher\",\n",
    "  \"mancherlei\",\n",
    "  \"mancherorts\",\n",
    "  \"manches\",\n",
    "  \"manchmal\",\n",
    "  \"mann\",\n",
    "  \"margin\",\n",
    "  \"massgebend\",\n",
    "  \"massgebende\",\n",
    "  \"massgebendem\",\n",
    "  \"massgebenden\",\n",
    "  \"massgebender\",\n",
    "  \"massgebendes\",\n",
    "  \"massgeblich\",\n",
    "  \"massgebliche\",\n",
    "  \"massgeblichem\",\n",
    "  \"massgeblichen\",\n",
    "  \"massgeblicher\",\n",
    "  \"mehr\",\n",
    "  \"mehrere\",\n",
    "  \"mehrerer\",\n",
    "  \"mehrfach\",\n",
    "  \"mehrmalig\",\n",
    "  \"mehrmaligem\",\n",
    "  \"mehrmaliger\",\n",
    "  \"mehrmaliges\",\n",
    "  \"mein\",\n",
    "  \"meine\",\n",
    "  \"meinem\",\n",
    "  \"meinen\",\n",
    "  \"meiner\",\n",
    "  \"meines\",\n",
    "  \"meinetwegen\",\n",
    "  \"meins\",\n",
    "  \"meist\",\n",
    "  \"meiste\",\n",
    "  \"meisten\",\n",
    "  \"meistens\",\n",
    "  \"meistenteils\",\n",
    "  \"meta\",\n",
    "  \"mich\",\n",
    "  \"mindestens\",\n",
    "  \"mir\",\n",
    "  \"mit\",\n",
    "  \"miteinander\",\n",
    "  \"mitgleich\",\n",
    "  \"mithin\",\n",
    "  \"mitnichten\",\n",
    "  \"mittels\",\n",
    "  \"mittelst\",\n",
    "  \"mitten\",\n",
    "  \"mittig\",\n",
    "  \"mitunter\",\n",
    "  \"mitwohl\",\n",
    "  \"mochte\",\n",
    "  \"möchte\",\n",
    "  \"möchten\",\n",
    "  \"möchtest\",\n",
    "  \"moechte\",\n",
    "  \"moeglich\",\n",
    "  \"moeglichst\",\n",
    "  \"moeglichste\",\n",
    "  \"moeglichstem\",\n",
    "  \"moeglichsten\",\n",
    "  \"moeglichster\",\n",
    "  \"mögen\",\n",
    "  \"möglich\",\n",
    "  \"mögliche\",\n",
    "  \"möglichen\",\n",
    "  \"möglicher\",\n",
    "  \"möglicherweise\",\n",
    "  \"möglichst\",\n",
    "  \"möglichste\",\n",
    "  \"möglichstem\",\n",
    "  \"möglichsten\",\n",
    "  \"möglichster\",\n",
    "  \"morgen\",\n",
    "  \"morgige\",\n",
    "  \"muessen\",\n",
    "  \"muesst\",\n",
    "  \"muesste\",\n",
    "  \"muss\",\n",
    "  \"müssen\",\n",
    "  \"musst\",\n",
    "  \"müßt\",\n",
    "  \"musste\",\n",
    "  \"müsste\",\n",
    "  \"mussten\",\n",
    "  \"müssten\",\n",
    "  \"nach\",\n",
    "  \"nachdem\",\n",
    "  \"nacher\",\n",
    "  \"nachher\",\n",
    "  \"nachhinein\",\n",
    "  \"nächste\",\n",
    "  \"nacht\",\n",
    "  \"naechste\",\n",
    "  \"naemlich\",\n",
    "  \"nahm\",\n",
    "  \"nämlich\",\n",
    "  \"naturgemaess\",\n",
    "  \"naturgemäss\",\n",
    "  \"natürlich\",\n",
    "  \"ncht\",\n",
    "  \"neben\",\n",
    "  \"nebenan\",\n",
    "  \"nehmen\",\n",
    "  \"nein\",\n",
    "  \"neu\",\n",
    "  \"neue\",\n",
    "  \"neuem\",\n",
    "  \"neuen\",\n",
    "  \"neuer\",\n",
    "  \"neuerdings\",\n",
    "  \"neuerlich\",\n",
    "  \"neuerliche\",\n",
    "  \"neuerlichem\",\n",
    "  \"neuerlicher\",\n",
    "  \"neuerliches\",\n",
    "  \"neues\",\n",
    "  \"neulich\",\n",
    "  \"neun\",\n",
    "  \"nicht\",\n",
    "  \"nichts\",\n",
    "  \"nichtsdestotrotz\",\n",
    "  \"nichtsdestoweniger\",\n",
    "  \"nie\",\n",
    "  \"niemals\",\n",
    "  \"niemand\",\n",
    "  \"niemandem\",\n",
    "  \"niemanden\",\n",
    "  \"niemandes\",\n",
    "  \"nimm\",\n",
    "  \"nimmer\",\n",
    "  \"nimmt\",\n",
    "  \"nirgends\",\n",
    "  \"nirgendwo\",\n",
    "  \"noch\",\n",
    "  \"noetigenfalls\",\n",
    "  \"nötigenfalls\",\n",
    "  \"nun\",\n",
    "  \"nur\",\n",
    "  \"nutzen\",\n",
    "  \"nutzt\",\n",
    "  \"nützt\",\n",
    "  \"nutzung\",\n",
    "  \"ob\",\n",
    "  \"oben\",\n",
    "  \"ober\",\n",
    "  \"oberen\",\n",
    "  \"oberer\",\n",
    "  \"oberhalb\",\n",
    "  \"oberste\",\n",
    "  \"obersten\",\n",
    "  \"oberster\",\n",
    "  \"obgleich\",\n",
    "  \"obs\",\n",
    "  \"obschon\",\n",
    "  \"obwohl\",\n",
    "  \"oder\",\n",
    "  \"oefter\",\n",
    "  \"oefters\",\n",
    "  \"off\",\n",
    "  \"offenkundig\",\n",
    "  \"offenkundige\",\n",
    "  \"offenkundigem\",\n",
    "  \"offenkundigen\",\n",
    "  \"offenkundiger\",\n",
    "  \"offenkundiges\",\n",
    "  \"offensichtlich\",\n",
    "  \"offensichtliche\",\n",
    "  \"offensichtlichem\",\n",
    "  \"offensichtlichen\",\n",
    "  \"offensichtlicher\",\n",
    "  \"offensichtliches\",\n",
    "  \"oft\",\n",
    "  \"öfter\",\n",
    "  \"öfters\",\n",
    "  \"oftmals\",\n",
    "  \"ohne\",\n",
    "  \"ohnedies\",\n",
    "  \"online\",\n",
    "  \"paar\",\n",
    "  \"partout\",\n",
    "  \"per\",\n",
    "  \"persoenlich\",\n",
    "  \"persoenliche\",\n",
    "  \"persoenlichem\",\n",
    "  \"persoenlicher\",\n",
    "  \"persoenliches\",\n",
    "  \"persönlich\",\n",
    "  \"persönliche\",\n",
    "  \"persönlicher\",\n",
    "  \"persönliches\",\n",
    "  \"pfui\",\n",
    "  \"ploetzlich\",\n",
    "  \"ploetzliche\",\n",
    "  \"ploetzlichem\",\n",
    "  \"ploetzlicher\",\n",
    "  \"ploetzliches\",\n",
    "  \"plötzlich\",\n",
    "  \"plötzliche\",\n",
    "  \"plötzlichem\",\n",
    "  \"plötzlicher\",\n",
    "  \"plötzliches\",\n",
    "  \"pro\",\n",
    "  \"quasi\",\n",
    "  \"reagiere\",\n",
    "  \"reagieren\",\n",
    "  \"reagiert\",\n",
    "  \"reagierte\",\n",
    "  \"recht\",\n",
    "  \"rechts\",\n",
    "  \"regelmäßig\",\n",
    "  \"reichlich\",\n",
    "  \"reichliche\",\n",
    "  \"reichlichem\",\n",
    "  \"reichlichen\",\n",
    "  \"reichlicher\",\n",
    "  \"restlos\",\n",
    "  \"restlose\",\n",
    "  \"restlosem\",\n",
    "  \"restlosen\",\n",
    "  \"restloser\",\n",
    "  \"restloses\",\n",
    "  \"richtiggehend\",\n",
    "  \"richtiggehende\",\n",
    "  \"richtiggehendem\",\n",
    "  \"richtiggehenden\",\n",
    "  \"richtiggehender\",\n",
    "  \"richtiggehendes\",\n",
    "  \"rief\",\n",
    "  \"rund\",\n",
    "  \"rundheraus\",\n",
    "  \"rundum\",\n",
    "  \"runter\",\n",
    "  \"sage\",\n",
    "  \"sagen\",\n",
    "  \"sagt\",\n",
    "  \"sagte\",\n",
    "  \"sagten\",\n",
    "  \"sagtest\",\n",
    "  \"sagtet\",\n",
    "  \"samt\",\n",
    "  \"sämtliche\",\n",
    "  \"sang\",\n",
    "  \"sangen\",\n",
    "  \"sattsam\",\n",
    "  \"schätzen\",\n",
    "  \"schätzt\",\n",
    "  \"schätzte\",\n",
    "  \"schätzten\",\n",
    "  \"scheinbar\",\n",
    "  \"scheinen\",\n",
    "  \"schlechter\",\n",
    "  \"schlicht\",\n",
    "  \"schlichtweg\",\n",
    "  \"schließlich\",\n",
    "  \"schlussendlich\",\n",
    "  \"schnell\",\n",
    "  \"schon\",\n",
    "  \"schreibe\",\n",
    "  \"schreiben\",\n",
    "  \"schreibens\",\n",
    "  \"schreiber\",\n",
    "  \"schwerlich\",\n",
    "  \"schwerliche\",\n",
    "  \"schwerlichem\",\n",
    "  \"schwerlichen\",\n",
    "  \"schwerlicher\",\n",
    "  \"schwerliches\",\n",
    "  \"schwierig\",\n",
    "  \"sechs\",\n",
    "  \"sect\",\n",
    "  \"sehe\",\n",
    "  \"sehen\",\n",
    "  \"sehr\",\n",
    "  \"sehrwohl\",\n",
    "  \"seht\",\n",
    "  \"sei\",\n",
    "  \"seid\",\n",
    "  \"seien\",\n",
    "  \"seiest\",\n",
    "  \"seiet\",\n",
    "  \"sein\",\n",
    "  \"seine\",\n",
    "  \"seinem\",\n",
    "  \"seinen\",\n",
    "  \"seiner\",\n",
    "  \"seines\",\n",
    "  \"seit\",\n",
    "  \"seitdem\",\n",
    "  \"seite\",\n",
    "  \"seiten\",\n",
    "  \"seither\",\n",
    "  \"selbe\",\n",
    "  \"selben\",\n",
    "  \"selber\",\n",
    "  \"selbst\",\n",
    "  \"selbstredend\",\n",
    "  \"selbstredende\",\n",
    "  \"selbstredendem\",\n",
    "  \"selbstredenden\",\n",
    "  \"selbstredender\",\n",
    "  \"selbstredendes\",\n",
    "  \"seltsamerweise\",\n",
    "  \"senke\",\n",
    "  \"senken\",\n",
    "  \"senkt\",\n",
    "  \"senkte\",\n",
    "  \"senkten\",\n",
    "  \"setzen\",\n",
    "  \"setzt\",\n",
    "  \"setzte\",\n",
    "  \"setzten\",\n",
    "  \"sich\",\n",
    "  \"sicher\",\n",
    "  \"sicherlich\",\n",
    "  \"sie\",\n",
    "  \"sieben\",\n",
    "  \"siebte\",\n",
    "  \"siehe\",\n",
    "  \"sieht\",\n",
    "  \"sind\",\n",
    "  \"singen\",\n",
    "  \"singt\",\n",
    "  \"so\",\n",
    "  \"sobald\",\n",
    "  \"sodaß\",\n",
    "  \"soeben\",\n",
    "  \"sofern\",\n",
    "  \"sofort\",\n",
    "  \"sog\",\n",
    "  \"sogar\",\n",
    "  \"sogleich\",\n",
    "  \"solange\",\n",
    "  \"solc\",\n",
    "  \"solc hen\",\n",
    "  \"solch\",\n",
    "  \"solche\",\n",
    "  \"solchem\",\n",
    "  \"solchen\",\n",
    "  \"solcher\",\n",
    "  \"solches\",\n",
    "  \"soll\",\n",
    "  \"sollen\",\n",
    "  \"sollst\",\n",
    "  \"sollt\",\n",
    "  \"sollte\",\n",
    "  \"sollten\",\n",
    "  \"solltest\",\n",
    "  \"solltet\",\n",
    "  \"somit\",\n",
    "  \"sondern\",\n",
    "  \"sonst\",\n",
    "  \"sonstig\",\n",
    "  \"sonstige\",\n",
    "  \"sonstigem\",\n",
    "  \"sonstiger\",\n",
    "  \"sonstwo\",\n",
    "  \"sooft\",\n",
    "  \"soviel\",\n",
    "  \"soweit\",\n",
    "  \"sowie\",\n",
    "  \"sowieso\",\n",
    "  \"sowohl\",\n",
    "  \"später\",\n",
    "  \"spielen\",\n",
    "  \"startet\",\n",
    "  \"startete\",\n",
    "  \"starteten\",\n",
    "  \"statt\",\n",
    "  \"stattdessen\",\n",
    "  \"steht\",\n",
    "  \"steige\",\n",
    "  \"steigen\",\n",
    "  \"steigt\",\n",
    "  \"stellenweise\",\n",
    "  \"stellenweisem\",\n",
    "  \"stellenweisen\",\n",
    "  \"stets\",\n",
    "  \"stieg\",\n",
    "  \"stiegen\",\n",
    "  \"such\",\n",
    "  \"suchen\",\n",
    "  \"tages\",\n",
    "  \"tat\",\n",
    "  \"tät\",\n",
    "  \"tatsächlich\",\n",
    "  \"tatsächlichen\",\n",
    "  \"tatsächlicher\",\n",
    "  \"tatsächliches\",\n",
    "  \"tatsaechlich\",\n",
    "  \"tatsaechlichen\",\n",
    "  \"tatsaechlicher\",\n",
    "  \"tatsaechliches\",\n",
    "  \"tausend\",\n",
    "  \"teile\",\n",
    "  \"teilen\",\n",
    "  \"teilte\",\n",
    "  \"teilten\",\n",
    "  \"tief\",\n",
    "  \"titel\",\n",
    "  \"toll\",\n",
    "  \"total\",\n",
    "  \"trage\",\n",
    "  \"tragen\",\n",
    "  \"trägt\",\n",
    "  \"trotzdem\",\n",
    "  \"trug\",\n",
    "  \"tun\",\n",
    "  \"tust\",\n",
    "  \"tut\",\n",
    "  \"txt\",\n",
    "  \"übel\",\n",
    "  \"über\",\n",
    "  \"überall\",\n",
    "  \"überallhin\",\n",
    "  \"überaus\",\n",
    "  \"überdies\",\n",
    "  \"überhaupt\",\n",
    "  \"überll\",\n",
    "  \"übermorgen\",\n",
    "  \"üblicherweise\",\n",
    "  \"übrig\",\n",
    "  \"übrigens\",\n",
    "  \"ueber\",\n",
    "  \"ueberall\",\n",
    "  \"ueberallhin\",\n",
    "  \"ueberaus\",\n",
    "  \"ueberdies\",\n",
    "  \"ueberhaupt\",\n",
    "  \"uebermorgen\",\n",
    "  \"ueblicherweise\",\n",
    "  \"uebrig\",\n",
    "  \"uebrigens\",\n",
    "  \"um\",\n",
    "  \"ums\",\n",
    "  \"umso\",\n",
    "  \"umstaendehalber\",\n",
    "  \"umständehalber\",\n",
    "  \"unbedingt\",\n",
    "  \"unbedingte\",\n",
    "  \"unbedingter\",\n",
    "  \"unbedingtes\",\n",
    "  \"und\",\n",
    "  \"unerhoert\",\n",
    "  \"unerhoerte\",\n",
    "  \"unerhoertem\",\n",
    "  \"unerhoerten\",\n",
    "  \"unerhoerter\",\n",
    "  \"unerhoertes\",\n",
    "  \"unerhört\",\n",
    "  \"unerhörte\",\n",
    "  \"unerhörtem\",\n",
    "  \"unerhörten\",\n",
    "  \"unerhörter\",\n",
    "  \"unerhörtes\",\n",
    "  \"ungefähr\",\n",
    "  \"ungemein\",\n",
    "  \"ungewoehnlich\",\n",
    "  \"ungewoehnliche\",\n",
    "  \"ungewoehnlichem\",\n",
    "  \"ungewoehnlichen\",\n",
    "  \"ungewoehnlicher\",\n",
    "  \"ungewoehnliches\",\n",
    "  \"ungewöhnlich\",\n",
    "  \"ungewöhnliche\",\n",
    "  \"ungewöhnlichem\",\n",
    "  \"ungewöhnlichen\",\n",
    "  \"ungewöhnlicher\",\n",
    "  \"ungewöhnliches\",\n",
    "  \"ungleich\",\n",
    "  \"ungleiche\",\n",
    "  \"ungleichem\",\n",
    "  \"ungleichen\",\n",
    "  \"ungleicher\",\n",
    "  \"ungleiches\",\n",
    "  \"unmassgeblich\",\n",
    "  \"unmassgebliche\",\n",
    "  \"unmassgeblichem\",\n",
    "  \"unmassgeblichen\",\n",
    "  \"unmassgeblicher\",\n",
    "  \"unmassgebliches\",\n",
    "  \"unmoeglich\",\n",
    "  \"unmoegliche\",\n",
    "  \"unmoeglichem\",\n",
    "  \"unmoeglichen\",\n",
    "  \"unmoeglicher\",\n",
    "  \"unmoegliches\",\n",
    "  \"unmöglich\",\n",
    "  \"unmögliche\",\n",
    "  \"unmöglichen\",\n",
    "  \"unmöglicher\",\n",
    "  \"unnötig\",\n",
    "  \"uns\",\n",
    "  \"unsaeglich\",\n",
    "  \"unsaegliche\",\n",
    "  \"unsaeglichem\",\n",
    "  \"unsaeglichen\",\n",
    "  \"unsaeglicher\",\n",
    "  \"unsaegliches\",\n",
    "  \"unsagbar\",\n",
    "  \"unsagbare\",\n",
    "  \"unsagbarem\",\n",
    "  \"unsagbaren\",\n",
    "  \"unsagbarer\",\n",
    "  \"unsagbares\",\n",
    "  \"unsäglich\",\n",
    "  \"unsägliche\",\n",
    "  \"unsäglichem\",\n",
    "  \"unsäglichen\",\n",
    "  \"unsäglicher\",\n",
    "  \"unsägliches\",\n",
    "  \"unse\",\n",
    "  \"unsem\",\n",
    "  \"unsen\",\n",
    "  \"unser\",\n",
    "  \"unsere\",\n",
    "  \"unserem\",\n",
    "  \"unseren\",\n",
    "  \"unserer\",\n",
    "  \"unseres\",\n",
    "  \"unserm\",\n",
    "  \"unses\",\n",
    "  \"unsre\",\n",
    "  \"unsrem\",\n",
    "  \"unsren\",\n",
    "  \"unsrer\",\n",
    "  \"unsres\",\n",
    "  \"unstreitig\",\n",
    "  \"unstreitige\",\n",
    "  \"unstreitigem\",\n",
    "  \"unstreitigen\",\n",
    "  \"unstreitiger\",\n",
    "  \"unstreitiges\",\n",
    "  \"unten\",\n",
    "  \"unter\",\n",
    "  \"unterbrach\",\n",
    "  \"unterbrechen\",\n",
    "  \"untere\",\n",
    "  \"unterem\",\n",
    "  \"unteres\",\n",
    "  \"unterhalb\",\n",
    "  \"unterste\",\n",
    "  \"unterster\",\n",
    "  \"unterstes\",\n",
    "  \"unwichtig\",\n",
    "  \"unzweifelhaft\",\n",
    "  \"unzweifelhafte\",\n",
    "  \"unzweifelhaftem\",\n",
    "  \"unzweifelhaften\",\n",
    "  \"unzweifelhafter\",\n",
    "  \"unzweifelhaftes\",\n",
    "  \"usw\",\n",
    "  \"usw.\",\n",
    "  \"vergangen\",\n",
    "  \"vergangene\",\n",
    "  \"vergangener\",\n",
    "  \"vergangenes\",\n",
    "  \"vermag\",\n",
    "  \"vermögen\",\n",
    "  \"vermutlich\",\n",
    "  \"vermutliche\",\n",
    "  \"vermutlichem\",\n",
    "  \"vermutlichen\",\n",
    "  \"vermutlicher\",\n",
    "  \"vermutliches\",\n",
    "  \"veröffentlichen\",\n",
    "  \"veröffentlicher\",\n",
    "  \"veröffentlicht\",\n",
    "  \"veröffentlichte\",\n",
    "  \"veröffentlichten\",\n",
    "  \"veröffentlichtes\",\n",
    "  \"verrate\",\n",
    "  \"verraten\",\n",
    "  \"verriet\",\n",
    "  \"verrieten\",\n",
    "  \"version\",\n",
    "  \"versorge\",\n",
    "  \"versorgen\",\n",
    "  \"versorgt\",\n",
    "  \"versorgte\",\n",
    "  \"versorgten\",\n",
    "  \"versorgtes\",\n",
    "  \"viel\",\n",
    "  \"viele\",\n",
    "  \"vielen\",\n",
    "  \"vieler\",\n",
    "  \"vielerlei\",\n",
    "  \"vieles\",\n",
    "  \"vielleicht\",\n",
    "  \"vielmalig\",\n",
    "  \"vielmals\",\n",
    "  \"vier\",\n",
    "  \"voellig\",\n",
    "  \"voellige\",\n",
    "  \"voelligem\",\n",
    "  \"voelligen\",\n",
    "  \"voelliger\",\n",
    "  \"voelliges\",\n",
    "  \"voelligst\",\n",
    "  \"vollends\",\n",
    "  \"völlig\",\n",
    "  \"völlige\",\n",
    "  \"völligem\",\n",
    "  \"völligen\",\n",
    "  \"völliger\",\n",
    "  \"völliges\",\n",
    "  \"völligst\",\n",
    "  \"vollstaendig\",\n",
    "  \"vollstaendige\",\n",
    "  \"vollstaendigem\",\n",
    "  \"vollstaendigen\",\n",
    "  \"vollstaendiger\",\n",
    "  \"vollstaendiges\",\n",
    "  \"vollständig\",\n",
    "  \"vollständige\",\n",
    "  \"vollständigem\",\n",
    "  \"vollständigen\",\n",
    "  \"vollständiger\",\n",
    "  \"vollständiges\",\n",
    "  \"vom\",\n",
    "  \"von\",\n",
    "  \"vor\",\n",
    "  \"voran\",\n",
    "  \"vorbei\",\n",
    "  \"vorgestern\",\n",
    "  \"vorher\",\n",
    "  \"vorherig\",\n",
    "  \"vorherige\",\n",
    "  \"vorherigem\",\n",
    "  \"vorheriger\",\n",
    "  \"vorne\",\n",
    "  \"vorüber\",\n",
    "  \"vorueber\",\n",
    "  \"wachen\",\n",
    "  \"waehrend\",\n",
    "  \"waehrenddessen\",\n",
    "  \"waere\",\n",
    "  \"während\",\n",
    "  \"währenddessen\",\n",
    "  \"wann\",\n",
    "  \"war\",\n",
    "  \"wär\",\n",
    "  \"wäre\",\n",
    "  \"waren\",\n",
    "  \"wären\",\n",
    "  \"warst\",\n",
    "  \"wart\",\n",
    "  \"warum\",\n",
    "  \"was\",\n",
    "  \"weder\",\n",
    "  \"weg\",\n",
    "  \"wegen\",\n",
    "  \"weil\",\n",
    "  \"weiß\",\n",
    "  \"weit\",\n",
    "  \"weiter\",\n",
    "  \"weitere\",\n",
    "  \"weiterem\",\n",
    "  \"weiteren\",\n",
    "  \"weiterer\",\n",
    "  \"weiteres\",\n",
    "  \"weiterhin\",\n",
    "  \"weitestgehend\",\n",
    "  \"weitestgehende\",\n",
    "  \"weitestgehendem\",\n",
    "  \"weitestgehenden\",\n",
    "  \"weitestgehender\",\n",
    "  \"weitestgehendes\",\n",
    "  \"weitgehend\",\n",
    "  \"weitgehende\",\n",
    "  \"weitgehendem\",\n",
    "  \"weitgehenden\",\n",
    "  \"weitgehender\",\n",
    "  \"weitgehendes\",\n",
    "  \"welche\",\n",
    "  \"welchem\",\n",
    "  \"welchen\",\n",
    "  \"welcher\",\n",
    "  \"welches\",\n",
    "  \"wem\",\n",
    "  \"wen\",\n",
    "  \"wenig\",\n",
    "  \"wenige\",\n",
    "  \"weniger\",\n",
    "  \"wenigstens\",\n",
    "  \"wenn\",\n",
    "  \"wenngleich\",\n",
    "  \"wer\",\n",
    "  \"werde\",\n",
    "  \"werden\",\n",
    "  \"werdet\",\n",
    "  \"weshalb\",\n",
    "  \"wessen\",\n",
    "  \"weswegen\",\n",
    "  \"wichtig\",\n",
    "  \"wie\",\n",
    "  \"wieder\",\n",
    "  \"wiederum\",\n",
    "  \"wieso\",\n",
    "  \"wieviel\",\n",
    "  \"wieviele\",\n",
    "  \"wievieler\",\n",
    "  \"wiewohl\",\n",
    "  \"will\",\n",
    "  \"willst\",\n",
    "  \"wir\",\n",
    "  \"wird\",\n",
    "  \"wirklich\",\n",
    "  \"wirklichem\",\n",
    "  \"wirklicher\",\n",
    "  \"wirkliches\",\n",
    "  \"wirst\",\n",
    "  \"wo\",\n",
    "  \"wobei\",\n",
    "  \"wodurch\",\n",
    "  \"wofuer\",\n",
    "  \"wofür\",\n",
    "  \"wogegen\",\n",
    "  \"woher\",\n",
    "  \"wohin\",\n",
    "  \"wohingegen\",\n",
    "  \"wohl\",\n",
    "  \"wohlgemerkt\",\n",
    "  \"wohlweislich\",\n",
    "  \"wolle\",\n",
    "  \"wollen\",\n",
    "  \"wollt\",\n",
    "  \"wollte\",\n",
    "  \"wollten\",\n",
    "  \"wolltest\",\n",
    "  \"wolltet\",\n",
    "  \"womit\",\n",
    "  \"womoeglich\",\n",
    "  \"womoegliche\",\n",
    "  \"womoeglichem\",\n",
    "  \"womoeglichen\",\n",
    "  \"womoeglicher\",\n",
    "  \"womoegliches\",\n",
    "  \"womöglich\",\n",
    "  \"womögliche\",\n",
    "  \"womöglichem\",\n",
    "  \"womöglichen\",\n",
    "  \"womöglicher\",\n",
    "  \"womögliches\",\n",
    "  \"woran\",\n",
    "  \"woraufhin\",\n",
    "  \"woraus\",\n",
    "  \"worden\",\n",
    "  \"worin\",\n",
    "  \"wuerde\",\n",
    "  \"wuerden\",\n",
    "  \"wuerdest\",\n",
    "  \"wuerdet\",\n",
    "  \"wurde\",\n",
    "  \"würde\",\n",
    "  \"wurden\",\n",
    "  \"würden\",\n",
    "  \"wurdest\",\n",
    "  \"würdest\",\n",
    "  \"wurdet\",\n",
    "  \"würdet\",\n",
    "  \"www\",\n",
    "  \"x\",\n",
    "  \"z.B.\",\n",
    "  \"zahlreich\",\n",
    "  \"zahlreichem\",\n",
    "  \"zahlreicher\",\n",
    "  \"zB\",\n",
    "  \"zb.\",\n",
    "  \"zehn\",\n",
    "  \"zeitweise\",\n",
    "  \"zeitweisem\",\n",
    "  \"zeitweisen\",\n",
    "  \"zeitweiser\",\n",
    "  \"ziehen\",\n",
    "  \"zieht\",\n",
    "  \"ziemlich\",\n",
    "  \"ziemliche\",\n",
    "  \"ziemlichem\",\n",
    "  \"ziemlichen\",\n",
    "  \"ziemlicher\",\n",
    "  \"ziemliches\",\n",
    "  \"zirka\",\n",
    "  \"zog\",\n",
    "  \"zogen\",\n",
    "  \"zu\",\n",
    "  \"zudem\",\n",
    "  \"zuerst\",\n",
    "  \"zufolge\",\n",
    "  \"zugleich\",\n",
    "  \"zuletzt\",\n",
    "  \"zum\",\n",
    "  \"zumal\",\n",
    "  \"zumeist\",\n",
    "  \"zumindest\",\n",
    "  \"zunächst\",\n",
    "  \"zunaechst\",\n",
    "  \"zur\",\n",
    "  \"zurück\",\n",
    "  \"zurueck\",\n",
    "  \"zusammen\",\n",
    "  \"zusehends\",\n",
    "  \"zuviel\",\n",
    "  \"zuviele\",\n",
    "  \"zuvieler\",\n",
    "  \"zuweilen\",\n",
    "  \"zwanzig\",\n",
    "  \"zwar\",\n",
    "  \"zwei\",\n",
    "  \"zweifelsfrei\",\n",
    "  \"zweifelsfreie\",\n",
    "  \"zweifelsfreiem\",\n",
    "  \"zweifelsfreien\",\n",
    "  \"zweifelsfreier\",\n",
    "  \"zweifelsfreies\",\n",
    "  \"zwischen\",\n",
    "  \"zwölf\",\n",
    "\n",
    "  # weitere stopwords \n",
    "  \"et\", \"et al\", \"al\",\n",
    "  \"exercise\", \n",
    "  \"projektpraktikum\",\n",
    "  \"ii\", \"qe ii\", \"qe\"\n",
    "  # \"strongly\", \"correlated\", \n",
    "  # \"englischen\",\n",
    "  # \"mögliche\",\n",
    "  # \"eingeführt\",\n",
    "  # \"milestones\",\n",
    "  # \"wesentlichen\",\n",
    "  # \"zwischen\",\n",
    "  # \"based\",\n",
    "  # \"diskutiert\",\n",
    "  # \"hergestellten\", \"einzelne\",\n",
    "  # \"theoretischen einweisung\",\n",
    "  # \"ökonomischen einsatzfeldern\",\n",
    "]\n",
    "\n",
    "sw = list(stopwords.get_stopwords(\"en\"))\n",
    "sw.extend(list(stopwords.get_stopwords(\"de\")))\n",
    "sw.extend(irrelevant_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d8f99",
   "metadata": {},
   "source": [
    "### Lemmatisierung\n",
    "Durch Lemmatisierung werden die Wörter in einheitliche Begriffe umgewandelt, sodass diese robuster werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60732494",
   "metadata": {},
   "source": [
    "## Nur zum Nachlesen: Konfiguration, Training und Evaluation des Topic Models\n",
    "Im Folgenden werden die einzelnen Schritte erläutert, wie man automatisiert ein, gemäss dem vordefinierten Goldstandard, möglichst performantes Modell erstellen kann. \n",
    "Dies dient lediglich als Nachschlagewerk, die Code-Zellen daher bitte __NICHT__ ausführen, während man das Topic Model automatisiert erstellen lassen will.\n",
    "Die Anwendung für das aktuelle Topic Modeling wird im nächsten Block durchgeführt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4167a6",
   "metadata": {},
   "source": [
    "### Konfiguration des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f45105",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "Der `CountVectorizer` ist ein wichtiges Werkzeug bei der Vorbereitung von Textdaten für ein Topic Model wie BERTopic. Ein Topic Model analysiert große Mengen an Text, um wiederkehrende Themen (Topics) zu erkennen. BERTopic kombiniert dabei Techniken aus der Natural Language Processing (NLP) mit Clustering-Methoden, um diese Themen zu extrahieren. Der `CountVectorizer` hilft dabei, den Text in eine numerische Darstellung umzuwandeln, die für das Modell nutzbar ist.\n",
    "\n",
    "- `stop_words=sw`:  \n",
    "  Stopwörter (z. B. \"und\", \"der\", \"ein\"), die keine inhaltliche Bedeutung tragen, werden entfernt. Dies stellt sicher, dass das Modell nur auf relevante Begriffe fokussiert ist und keine irrelevanten Wörter in die Themenbildung einfließen.\n",
    "\n",
    "- `token_pattern=r'\\b\\w+\\b'`:  \n",
    "  Der reguläre Ausdruck sorgt dafür, dass nur ganze Wörter als Token betrachtet werden. Sonderzeichen oder isolierte Zahlen werden ausgeschlossen, da sie selten zur inhaltlichen Bedeutung beitragen.\n",
    "\n",
    "- `ngram_range=(1, 3)`:  \n",
    "  Es werden nicht nur einzelne Wörter (1-Gramme), sondern auch Wortkombinationen aus zwei oder drei aufeinanderfolgenden Wörtern (n-Gramme) berücksichtigt. Diese Phrasen wie \"künstliche Intelligenz\" oder \"Datenanalyse\" helfen BERTopic, kontextbezogene und aussagekräftige Themen zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f166527",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=sw,  # Entfernt Stopwörter basierend auf der angegebenen Liste (sw)\n",
    "    token_pattern=r'\\b\\w+\\b',  # Extrahiert nur ganze Wörter, d. h. keine Sonderzeichen oder Zahlen\n",
    "    ngram_range=(1, 3)  # Erstellt 1-Gramme (einzelne Wörter) bis 3-Gramme (Wortgruppen aus bis zu 3 aufeinanderfolgenden Wörtern)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68b166",
   "metadata": {},
   "source": [
    "#### EmbeddingSettings\n",
    "\n",
    "Die `EmbeddingSettings` definieren die Konfiguration für die Generierung von Text-Embeddings, die zur numerischen Darstellung von Textdaten verwendet werden. Diese Embeddings fassen semantische Ähnlichkeiten und Bedeutungen von Texten in einem Vektorraum zusammen und dienen als Grundlage für weitere Analysen, z. B. Clustering oder Themenmodellierung.\n",
    "\n",
    "### Parameter der EmbeddingSettings\n",
    "\n",
    "- `embedding_model`:  \n",
    "  Gibt das Modell an, das zur Generierung der Embeddings verwendet wird. Hier wird die Klasse `SentenceTransformer` genutzt, die leistungsstarke vortrainierte Transformer-Modelle für die Textverarbeitung unterstützt.\n",
    "\n",
    "- `model_name_or_path`:  \n",
    "  Gibt den Pfad oder Namen des vortrainierten Modells an. In diesem Fall wird das Modell `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` verwendet, das speziell für mehrsprachige Anwendungen optimiert ist. Dieses Modell erzeugt kompakte und semantisch aussagekräftige Embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b634f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbeddingSettings = {\n",
    "    \"embedding_model\": SentenceTransformer,\n",
    "    \"model_name_or_path\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1b815",
   "metadata": {},
   "source": [
    "#### UMAPSettings\n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) ist eine Technik zur Dimensionsreduktion, die häufig in der Verarbeitung von hochdimensionalen Daten, wie Text- oder Bilddaten, eingesetzt wird. Sie projiziert mehrdimensionale Daten in einen Raum mit geringerer Dimension, um Muster und Strukturen leichter zu erkennen. Die `UMAPSettings` definieren die Parameter, die das Verhalten und die Genauigkeit dieser Projektion steuern.\n",
    "\n",
    "\n",
    "- `n_neighbors=15`:  \n",
    "  Gibt die Anzahl der nächsten Nachbarn an, die für jeden Punkt berücksichtigt werden. Ein höherer Wert fokussiert auf größere Strukturen in den Daten, während ein niedrigerer Wert stärker lokale Muster betont.\n",
    "\n",
    "- `n_components=5`:  \n",
    "  Legt die Dimension des reduzierten Raumes fest. In diesem Fall werden die Daten in 5 Dimensionen projiziert, was hilft, wesentliche Eigenschaften der Daten zu erhalten.\n",
    "\n",
    "- `min_dist=0.1`:  \n",
    "  Bestimmt, wie nah Punkte im projizierten Raum beieinander liegen können. Ein niedriger Wert führt zu eng gepackten Clustern, während ein höherer Wert eine gleichmäßigere Verteilung ermöglicht.\n",
    "\n",
    "- `metric=\"cosine\"`:  \n",
    "  Gibt die Distanzmetrik an, die verwendet wird, um die Ähnlichkeit zwischen Punkten zu berechnen. Der Kosinusabstand ist besonders geeignet für Textdaten oder hochdimensionale Vektoren.\n",
    "\n",
    "- `random_state=13`:  \n",
    "  Definiert einen Seed-Wert für den Zufallszahlengenerator, um reproduzierbare Ergebnisse sicherzustellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abbbfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAPSettings = {\n",
    "    \"n_neighbors\": 15,\n",
    "    \"n_components\": 5,\n",
    "    \"min_dist\": 0.1,\n",
    "    \"metric\": \"cosine\",\n",
    "    \"random_state\": 13\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba02d1c",
   "metadata": {},
   "source": [
    "#### HDBSCANSettings\n",
    "\n",
    "HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) ist ein Clustering-Algorithmus, der besonders gut mit komplexen Datensätzen umgehen kann. Er identifiziert Cluster auf der Basis von Dichte und ermöglicht es, Datenpunkte als *Rauschen* zu klassifizieren, wenn sie nicht zu einem Cluster gehören. Die `HDBSCANSettings` definieren die Parameter, die das Verhalten des Algorithmus steuern.\n",
    "\n",
    "##### Parameter der HDBSCANSettings\n",
    "\n",
    "- `min_samples=10`:  \n",
    "  Gibt die minimale Anzahl von Datenpunkten an, die in der Nachbarschaft eines Punktes vorhanden sein müssen, damit er als Kernpunkt eines Clusters gilt. Ein höherer Wert macht den Algorithmus empfindlicher gegenüber Rauschen.\n",
    "\n",
    "- `gen_min_span_tree=True`:  \n",
    "  Erstellt einen minimalen `span tree`, der die hierarchische Struktur der Cluster visualisiert. Dies ist nützlich für die Analyse und Interpretation der Ergebnisse.\n",
    "\n",
    "- `prediction_data=True`:  \n",
    "  Ermöglicht die Generierung zusätzlicher Daten, die für die spätere Zuordnung neuer Punkte zu den Clustern verwendet werden können.\n",
    "\n",
    "- `min_cluster_size=100`:  \n",
    "  Gibt die minimale Größe eines Clusters an. Cluster mit weniger Datenpunkten werden als Rauschen betrachtet und nicht berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffde44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDBSCANSettings = {\n",
    "    \"min_samples\": 3,\n",
    "    \"gen_min_span_tree\": True,\n",
    "    \"prediction_data\": True,\n",
    "    \"min_cluster_size\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1ec40",
   "metadata": {},
   "source": [
    "#### BERTopicSettings\n",
    "\n",
    "Die `BERTopicSettings` definieren die Konfiguration des BERTopic-Modells, das zur Themenmodellierung verwendet wird. BERTopic identifiziert wiederkehrende Themen in Textdaten und ermöglicht eine flexible Anpassung der Ergebnisse durch verschiedene Parameter.\n",
    "\n",
    "#### Parameter der BERTopicSettings\n",
    "\n",
    "- `top_n_words=10`:  \n",
    "  Gibt an, wie viele Schlüsselwörter pro Thema angezeigt werden. Ein höherer Wert liefert detailliertere Informationen zu den Themen.\n",
    "\n",
    "- `language=\"multilingual\"`:  \n",
    "  Setzt die Sprache für die Verarbeitung von Textdaten. Mit \"multilingual\" wird sichergestellt, dass Texte in mehreren Sprachen unterstützt werden.\n",
    "\n",
    "- `n_gram_range=(1, 4)`:  \n",
    "  Bestimmt den Bereich der n-Gramme (z. B. einzelne Wörter bis zu Vier-Wort-Kombinationen), die für die Themenanalyse berücksichtigt werden.\n",
    "\n",
    "- `min_topic_size=100`:  \n",
    "  Legt die minimale Anzahl von Dokumenten fest, die ein Thema enthalten muss, damit es berücksichtigt wird. Kleinere Mindestgrößen ermöglichen es, mehr spezialisierte Themen zu erkennen.\n",
    "\n",
    "- `calculate_probabilities=True`:  \n",
    "  Aktiviert die Berechnung von Wahrscheinlichkeiten, die die Zugehörigkeit von Dokumenten zu bestimmten Themen darstellen.\n",
    "\n",
    "- `verbose=True`:  \n",
    "  Aktiviert detaillierte Konsolenausgaben, die den Fortschritt des Modells anzeigen.\n",
    "\n",
    "- `nr_topics=20`:  \n",
    "  Setzt die Anzahl der finalen Themen auf 20. Diese Begrenzung wird durch die Reduktion ähnlicher Themen erreicht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc1e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTopicSettings = {\n",
    "    \"top_n_words\": 10,  # Mehr Top-Wörter, um relevantere Themen zu erfassen\n",
    "    \"language\": \"multilingual\",\n",
    "    \"n_gram_range\": (1, 3),  # Erweiterung des n-Gram-Bereichs\n",
    "    \"min_topic_size\": 10,  # Kleinere Mindestgröße der Themen\n",
    "    \"calculate_probabilities\": True,\n",
    "    \"verbose\": True,\n",
    "    \"nr_topics\": None,  # Festlegung der Anzahl der Themen\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d775dfb",
   "metadata": {},
   "source": [
    "#### BERTopic: Initialisieren und trainieren\n",
    "\n",
    "Im folgenden wird das Modell initialisiert.\n",
    "\n",
    "##### Parameter BERTopic\n",
    "\n",
    "- `ctfidf_model`:  \n",
    "  Ein ClassTfidfTransformer wird mit dem Parameter `reduce_frequent_words=True` initialisiert. Dies reduziert den Einfluss hochfrequenter Wörter, die zusätzlich zu Stopwörtern das Modell verzerren könnten.\n",
    "\n",
    "- `topic_model`:  \n",
    "  Initialisierung des BERTopic-Modells mit verschiedenen benutzerdefinierten Einstellungen:\n",
    "  - `ctfidf_model=ctfidf_model`:  \n",
    "    Verwendet das zuvor definierte ClassTfidfTransformer-Modell, um hochfrequente Wörter zu behandeln.\n",
    "  - `vectorizer_model=vectorizer`:  \n",
    "    Übergibt den zuvor definierten CountVectorizer, der für die Tokenisierung und n-Gramm-Erstellung genutzt wird.\n",
    "  - `embedding_model=EmbeddingSettings[\"embedding_model\"](EmbeddingSettings[\"model_name_or_path\"])`:  \n",
    "    Erstellt ein Text-Embedding-Modell basierend auf den in EmbeddingSettings angegebenen Parametern.\n",
    "  - `umap_model=UMAP(**UMAPSettings)`:  \n",
    "    Nutzt UMAP zur Dimensionsreduktion mit den vorher definierten Einstellungen in UMAPSettings.\n",
    "  - `hdbscan_model=HDBSCAN(**HDBSCANSettings)`:  \n",
    "    Führt das Clustering mit HDBSCAN durch, basierend auf den Einstellungen in HDBSCANSettings.\n",
    "  - `**BERTopicSettings`:  \n",
    "    Übernimmt zusätzliche Parameter aus BERTopicSettings, wie die Anzahl der Themen oder die Sprache.\n",
    "\n",
    "- Modelltraining und Transformation:  \n",
    "  Der Datensatz wird durch das `fit_transform`-Verfahren verarbeitet:\n",
    "  - `topics`: Enthält die identifizierten Themen für jedes Dokument.\n",
    "  - `probs`: Liefert die Wahrscheinlichkeiten, mit denen ein Dokument zu einem Thema gehört.\n",
    "\n",
    "- Themenanalyse:  \n",
    "  `topic_model.get_topic_info()` gibt eine Übersicht der erkannten Themen, deren Häufigkeit und Repräsentation zurück. Hier werden die Top 50 Themen extrahiert und nach ihrer ID aufgelistet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fef1cd",
   "metadata": {},
   "source": [
    "#### Definition Objective\n",
    "Hier wird das Package \"optuna\" verwendet, durch welches die zu testenden Parameter festgelegt werden können und anhand einer definierten Metrik optimiert werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782d9b0",
   "metadata": {},
   "source": [
    "## Anwendung: Konfiguration, Training und Evaluation des Topic Models\n",
    "Hier muss alles innerhalb einer einzigen Code-Zelle erfolgen, da bei allen Konfigurationen variable Parameter vorkommen und wir diese durch das optuna-Package optimieren wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8949c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im Objective werden die verschiedenen Parameter-Settings gesetzt, über welche man optimieren möchte\n",
    "def objective(trial):\n",
    "\n",
    "  try:\n",
    "    # Embedding Settings\n",
    "    embedding_model_name = trial.suggest_categorical(\"embedding_model\", [\"paraphrase-multilingual-MiniLM-L12-v2\", \"paraphrase-mpnet-base-v2\"])\n",
    "    # UMAP Settings\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 4, 14, 2)\n",
    "    min_dist = trial.suggest_float(\"min_dist\", 0.0, 0.2, step = 0.1)\n",
    "    n_components = trial.suggest_int(\"n_components\", 3, 8, 1)\n",
    "    # HDBSCAN Settings\n",
    "    min_cluster_size = trial.suggest_int(\"min_cluster_size\", 3, 8, 1)\n",
    "    #min_samples = trial.suggest_int(\"min_samples\", 3, 10, 1)\n",
    "    # BERTopic Settings\n",
    "    #nr_topics = trial.suggest_categorical(\"nr_topics\", [5, 6, 7, 8, 9, 10, 15])\n",
    "    diversity = trial.suggest_float(\"diversity\", 0.0, 0.2, step = 0.1)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Konfiguration\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # CountVectorizer\n",
    "    vectorizer = CountVectorizer(\n",
    "      stop_words=sw,  # Entfernt Stopwörter basierend auf der angegebenen Liste (sw)\n",
    "      token_pattern=r'\\b\\w+\\b',  # Extrahiert nur ganze Wörter, d. h. keine Sonderzeichen oder Zahlen\n",
    "      ngram_range=(1, 3)  # Erstellt 1-Gramme (einzelne Wörter) bis 3-Gramme (Wortgruppen aus bis zu 3 aufeinanderfolgenden Wörtern)\n",
    "    )\n",
    "\n",
    "    # Embedding Settings  \n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    \n",
    "    # UMAP Settings\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=\"cosine\", random_state=13)\n",
    "\n",
    "    # HDBSCAN Settings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True, prediction_data=True)\n",
    "\n",
    "    # Representation Settings\n",
    "    representation_model = MaximalMarginalRelevance(diversity=diversity)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Training\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # BERTopic initialisieren\n",
    "    topic_model = BERTopic(\n",
    "      embedding_model=embedding_model,\n",
    "      #min_topic_size=10,\n",
    "      #nr_topics=nr_topics, \n",
    "      language=\"multilingual\",\n",
    "      umap_model=umap_model,\n",
    "      vectorizer_model=vectorizer,\n",
    "      hdbscan_model=hdbscan_model,\n",
    "      top_n_words = 30,\n",
    "      representation_model=representation_model\n",
    "    )\n",
    "\n",
    "    # BERTopic trainieren\n",
    "    topic_model_quanten = topic_model.fit(docs)\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Evaluation\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # BERTopic auf Test-Daten anwenden\n",
    "    topics, probs = topic_model_quanten.transform(test_set)\n",
    "    print(topic_model_quanten.get_topic_freq())\n",
    "\n",
    "    # Outlier reduzieren\n",
    "    topics = topic_model_quanten.reduce_outliers(test_set, topics)\n",
    "\n",
    "    # Resultierende Topic-Nummern mit den Representations (= relevante Begriffe) zu einem Datensatz kombinieren\n",
    "    dataframe_with_results_left = pd.DataFrame(topics, columns = [\"Topic\"])\n",
    "    dataframe_with_results_right = pd.DataFrame(topic_model_quanten.get_topic_info().set_index('Topic')[['Representation']])\n",
    "    dataframe_with_results = dataframe_with_results_left.join(dataframe_with_results_right, on=\"Topic\")\n",
    "\n",
    "    # Goldstandard (Ground Truth) mit den Ergebnissen abgleichen und Score berechnen (Score = Anteil korrekter Topic-Zuweisungen)\n",
    "    row_number = 0\n",
    "    metric = 0\n",
    "    while row_number < len(ground_truth):\n",
    "      # Den Goldstandard in eine Liste von Keywords umwandeln\n",
    "      ground_truth_current_iteration = ground_truth[row_number].split(\", \")\n",
    "      result_current_iteration = dataframe_with_results.at[row_number, \"Representation\"]\n",
    "\n",
    "      # Überprüfen, ob irgendein Begriff aus dem Resultat im Goldstandard zum Text vorkommt (1 = ja, 0 = nein)\n",
    "      if any(element in result_current_iteration for element in ground_truth_current_iteration):\n",
    "              metric += 1\n",
    "      else: metric += 0\n",
    "\n",
    "      row_number = row_number+1\n",
    "\n",
    "      print(result_current_iteration)\n",
    "      print(ground_truth_current_iteration)\n",
    "      print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    metric_score = metric/row_number\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Ergebnis printen und Score returnen\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Ergebnis printen\n",
    "    #print(\"Modell evaluiert mit einem Score von \", metric_score, \". \" \\\n",
    "    #\"Verwendete Parameter: embedding model: \", embedding_model_name, \", nr_topics: \", nr_topics, \", n_neighbors: \", n_neighbors, \", min_dist: \", min_dist,\n",
    "    #\", n_components: \", n_components, \", min_cluster_size: \", min_cluster_size)\n",
    "  \n",
    "    return metric_score \n",
    "  \n",
    "  except Exception as e:\n",
    "      print(\"Trial wird aufgrund eines Errors übersprungen\")\n",
    "      print(\"Verwendete Parameter: embedding model: \", embedding_model_name, \", n_neighbors: \", n_neighbors, \", min_dist: \", min_dist,\\\n",
    "      \", n_components: \", n_components, \", min_cluster_size: \", min_cluster_size)\n",
    "      print(e)\n",
    "      raise optuna.TrialPruned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd575faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-06 10:36:05,159] A new study created in memory with name: no-name-8aac78de-c363-47a5-8d0a-dd56bc6a9fd8\n",
      "[I 2025-11-06 10:37:01,578] Trial 0 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial wird aufgrund eines Errors übersprungen\n",
      "Verwendete Parameter: embedding model:  paraphrase-multilingual-MiniLM-L12-v2 , n_neighbors:  6 , min_dist:  0.0 , n_components:  4 , min_cluster_size:  3\n",
      "name 'test_set' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-06 10:37:38,236] Trial 1 failed with parameters: {'embedding_model': 'paraphrase-mpnet-base-v2', 'n_neighbors': 10, 'min_dist': 0.1, 'n_components': 8, 'min_cluster_size': 3, 'diversity': 0.0} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\mhu\\AppData\\Local\\Temp\\ipykernel_18168\\2879465152.py\", line 61, in objective\n",
      "    topic_model_quanten = topic_model.fit(docs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\_bertopic.py\", line 364, in fit\n",
      "    self.fit_transform(documents=documents, embeddings=embeddings, y=y, images=images)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\_bertopic.py\", line 431, in fit_transform\n",
      "    embeddings = self._extract_embeddings(\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\_bertopic.py\", line 3677, in _extract_embeddings\n",
      "    embeddings = self.embedding_model.embed_documents(documents, verbose=verbose)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\backend\\_base.py\", line 62, in embed_documents\n",
      "    return self.embed(document, verbose)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py\", line 65, in embed\n",
      "    embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 623, in encode\n",
      "    out_features = self.forward(features, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 690, in forward\n",
      "    input = module(input, **module_kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 385, in forward\n",
      "    output_states = self.auto_model(**trans_features, **kwargs, return_dict=False)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\", line 544, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\", line 334, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\", line 304, in forward\n",
      "    layer_output = self.output(intermediate_output, attention_output)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\", line 271, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-06 10:37:38,250] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    257\u001b[0m ):\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[10], line 61\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     48\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic(\n\u001b[0;32m     49\u001b[0m   embedding_model\u001b[38;5;241m=\u001b[39membedding_model,\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;66;03m#min_topic_size=10,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m   representation_model\u001b[38;5;241m=\u001b[39mrepresentation_model\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# BERTopic trainieren\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m topic_model_quanten \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# BERTopic auf Test-Daten anwenden\u001b[39;00m\n\u001b[0;32m     69\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m topic_model_quanten\u001b[38;5;241m.\u001b[39mtransform(test_set)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\_bertopic.py:364\u001b[0m, in \u001b[0;36mBERTopic.fit\u001b[1;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    324\u001b[0m     documents: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m     y: Union[List[\u001b[38;5;28mint\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    328\u001b[0m ):\n\u001b[0;32m    329\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics.\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\_bertopic.py:431\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[0;32m    429\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Transforming documents to embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m select_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m--> 431\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Completed \u001b[39m\u001b[38;5;130;01m\\u2713\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\_bertopic.py:3677\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[1;34m(self, documents, images, method, verbose)\u001b[0m\n\u001b[0;32m   3675\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39membed_words(words\u001b[38;5;241m=\u001b[39mdocuments, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m   3676\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 3677\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3678\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m documents[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to use an embedding model that can either embed documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor images depending on which you want to embed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3682\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\backend\\_base.py:62\u001b[0m, in \u001b[0;36mBaseEmbedder.embed_documents\u001b[1;34m(self, document, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, document: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    matrix of embeddings.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py:65\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    matrix of embeddings.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:385\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    383\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 385\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    386\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:544\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    543\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds)\n\u001b[1;32m--> 544\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    553\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:334\u001b[0m, in \u001b[0;36mMPNetEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    332\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m--> 334\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    335\u001b[0m     hidden_states,\n\u001b[0;32m    336\u001b[0m     attention_mask,\n\u001b[0;32m    337\u001b[0m     head_mask[i],\n\u001b[0;32m    338\u001b[0m     position_bias,\n\u001b[0;32m    339\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    341\u001b[0m )\n\u001b[0;32m    342\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:304\u001b[0m, in \u001b[0;36mMPNetLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[0;32m    303\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 304\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:271\u001b[0m, in \u001b[0;36mMPNetOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 271\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    273\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mhu\\miniconda3\\envs\\bertopic-env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction = \"maximize\")\n",
    "study.optimize(objective, n_trials = 100)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
