{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da160223",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "## Pakete und Daten laden\n",
    "\n",
    "In einem ersten Schritt laden wir die nöten Pakete und laden die Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800a438-5e88-4c45-a726-bda7eeecac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stopwords\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import os\n",
    "import openpyxl\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from pathlib import Path\n",
    "\n",
    "target = Path(\"C:/Users/mhu/Documents/github/topic_model_it\")\n",
    "os.chdir(target)\n",
    "\n",
    "data_model_1 = pd.read_csv(\"data/informatikkurse.csv\")\n",
    "print(data_model_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ae8bd",
   "metadata": {},
   "source": [
    "## Reproduzierbarkeit durch Seeds\n",
    "Im folgenden fixieren wir die Zufallszahlen-Generatoren aller beteiligten Bibliotheken auf den Wert 11. Dies stellt sicher, dass die Experimente bei jedem Durchlauf identische Ergebnisse liefern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9a8dc-d866-44cf-86bc-e26fca5c9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11  # Initialisiert den Seed-Wert für reproduzierbare Ergebnisse\n",
    "np.random.seed(seed)  # Setzt den Seed für NumPy-Zufallszahlengeneratoren\n",
    "random.seed(seed)  # Setzt den Seed für den Python-eigenen Zufallszahlengenerator\n",
    "torch.manual_seed(seed)  # Setzt den Seed für PyTorch-Zufallszahlen\n",
    "if torch.cuda.is_available():  # Überprüft, ob CUDA (GPU-Unterstützung) verfügbar ist\n",
    "    torch.cuda.manual_seed_all(seed)  # Setzt den Seed für alle CUDA-Zufallszahlen (für GPU-Berechnungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e14ed",
   "metadata": {},
   "source": [
    "## Docs\n",
    "\n",
    "Wir generieren als nächsten die Dokumente, die das Eingangsmaterial für das Topic-Model bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cb0b180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veranstaltung_titel', 'kursbeschreibung', 'titel_kursbesch']\n"
     ]
    }
   ],
   "source": [
    "# Anzeige der Spaltennamen von data_model_1\n",
    "print(data_model_1.columns.tolist())\n",
    "\n",
    "data_model_1 = data[[\"veranstaltung_titel\", \"kursbeschreibung\"]].copy()\n",
    "data_model_1.head()\n",
    "\n",
    "data_model_1 = data_model_1.apply(lambda x: x.fillna('') if x.dtype == 'O' else x)  # Ersetzt fehlende Werte durch leere Strings in Objektspalten (Strings) und belässt numerische Spalten unverändert\n",
    "data_model_1['titel_kursbesch'] = data_model_1['veranstaltung_titel'] + ' ' + data_model_1['kursbeschreibung']  # Kombiniert die Spalten \"titel\" und \"kursbeschreibung\" zu einer neuen Spalte \"titel_kursbesch\"\n",
    "docs = data_model_1['titel_kursbesch'].tolist()  # Konvertiert die Inhalte der Spalte \"titel_kursbesch\" in eine Liste von Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cac53-81bc-4784-8716-0108d3bfdf31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d74ac8-6f38-4603-8d4c-d0e7d33be06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stopwords_config\n",
    "\n",
    "irrelevant_terms = stopwords_config.irrelevant_terms\n",
    "\n",
    "sw = list(stopwords.get_stopwords(\"en\"))\n",
    "sw.extend(list(stopwords.get_stopwords(\"de\")))\n",
    "sw.extend(irrelevant_terms)\n",
    "irrelevant_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4740b1",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Wir definieren unser Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d14e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7373be7-520a-4bc9-84cf-2b6e25ef95bd",
   "metadata": {},
   "source": [
    "## Model-Settings\n",
    "Konfiguration frei wählbar (einfach im Code unten anpassen).\n",
    "Hier kann eine erste explorative Untersuchung durchgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b52a78b-fa5d-4d36-8c40-86f509ba02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "  stop_words=sw,  # Entfernt Stopwörter basierend auf der angegebenen Liste (sw)\n",
    "  token_pattern=r'\\b\\w+\\b',  # Extrahiert nur ganze Wörter, d. h. keine Sonderzeichen oder Zahlen\n",
    "  ngram_range=(1, 3)  # Erstellt 1-Gramme (einzelne Wörter) bis 3-Gramme (Wortgruppen aus bis zu 3 aufeinanderfolgenden Wörtern)\n",
    ")\n",
    "\n",
    "# Embedding Settings  \n",
    "embedding_model = SentenceTransformer(\n",
    "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# UMAP Settings\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10,\n",
    "    n_components=10,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=seed\n",
    ")\n",
    "# HDBSCAN Settings\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    cluster_selection_epsilon=0.2,\n",
    "    prediction_data=True\n",
    ")\n",
    "# Representation Settings\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9dc45-080c-4e8d-99b3-978cfc0e8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic initialisieren\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  #min_topic_size=10,\n",
    "  nr_topics=25, \n",
    "  language=\"multilingual\",\n",
    "  umap_model=umap_model,\n",
    "  vectorizer_model=vectorizer,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  top_n_words = 15,\n",
    "  representation_model=representation_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99727b96-036b-4d95-81d0-93f224e36335",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17203a0-5422-4148-98a9-8242e72155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_quanten = topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e56f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrekt & knapp\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from bertopic.representation import OpenAI as OpenAIRep\n",
    "\n",
    " \n",
    "client = OpenAIClient()  # nutzt OPENAI_API_KEY aus der Umgebung\n",
    "rep = OpenAIRep(client=client, model=\"gpt-4o-mini\", delay_in_seconds=5)\n",
    "\n",
    "# nutze dieselben Texte wie beim Fit (z. B. `docs`)\n",
    "topic_model.update_topics(docs, representation_model=rep)\n",
    "topic_model.get_topic_info()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378b00b-4b85-4392-b13c-89eab25ef72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BERTopic auf Test-Daten anwenden\n",
    "topics, probs = topic_model_quanten.transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e301c-8990-423d-b6f6-4deffde936b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_quanten.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd73b8-2431-4172-b9d2-26066daf3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier reduzieren\n",
    "topics = topic_model_quanten.reduce_outliers(test_docs, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3ce21-6345-404a-b054-e5c1e3c676c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a8df8-4f80-429a-8309-7640a38eb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultierende Topic-Nummern mit den Representations (= relevante Begriffe) zu einem Datensatz kombinieren\n",
    "dataframe_with_results_left = pd.DataFrame(topics, columns = [\"Topic\"])\n",
    "dataframe_with_results_right = pd.DataFrame(topic_model_quanten.get_topic_info().set_index('Topic')[['Representation']])\n",
    "dataframe_with_results = dataframe_with_results_left.join(dataframe_with_results_right, on=\"Topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a8b5c-56dd-46ed-9ecf-0d5a2d25cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_number = 0\n",
    "metric = 0\n",
    "while row_number < len(ground_truth):\n",
    "  # Den Goldstandard in eine Liste von Keywords umwandeln\n",
    "  ground_truth_current_iteration = ground_truth[row_number].split(\", \")\n",
    "  result_current_iteration = dataframe_with_results.at[row_number, \"Representation\"]\n",
    "  # Überprüfen, ob irgendein Begriff aus dem Resultat im Goldstandard zum Text vorkommt (1 = ja, 0 = nein)\n",
    "  if any(element in result_current_iteration for element in ground_truth_current_iteration):\n",
    "      metric += 1\n",
    "  else: \n",
    "      metric += 0\n",
    "      print(result_current_iteration)\n",
    "      print(ground_truth_current_iteration)\n",
    "      print(\"--------------------------------------------------------------------------------------------------\")\n",
    "  row_number = row_number+1\n",
    "\n",
    "metric_score = metric/row_number\n",
    "print(metric_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
